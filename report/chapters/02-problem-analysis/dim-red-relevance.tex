\section{Linear versus nonlinear methods}\label{sec:linear-vs-nonlinear}

In this section we will explore dimensionality reduction more. More specifically, in this project we will distinguish between linear and nonlinear methods. According to John A. Lee, there are several distinctions that can be made for dimensionality reduction methods\cite{nonlinear-dim-red-chapter-two}. We will not focus on them, but will choose to classify them as linear and nonlinear because it is the most straightforward way of classifying them. Nonlinear methods are much more complex than the linear ones, and also more computationally expensive~\cite{nonlinear-dim-red-chapter-two}. The details regarding the difference between the methods will be presented in section~\ref{sec:dimensionality-reduction}.

\subsection{Applications of dimensionality reduction methods}

Dimensionality reduction has many different applications, and for different applications, different methods can prove to be more suitable. In this section we will discuss some of the most common applications of dimensionality reduction methods. An example of the applications of dimensionality reduction, can be seen in \cite{sarwar2000application}, that discusses the use of dimensionality reduction in recommender systems. Using dimensionality reduction methods to help the scalability of the system by reducing the amount of data run on the system, and also to improve the quality of the recommendations. Additionally, dimensionality reduction can also be used in applications, such as image compression, visualization of high dimensional data, and feature extraction. Showing that is has many different applications, and that it is a very useful tool.


\subsection{Results and differences of linear and nonlinear methods}

The use of the linear and nonlinear dimensionality reduction methods can among others be seen in~\cite{dimensionality-reduction-comparative-review, tennenbaum}, where the methods have been tested on artificial and real-world datasets. As an example, it is shown that artificial datasets such as the swiss-roll, show that linear methods fail to find the intrinsic dimensionality of the data, as opposed to nonlinear methods~\cite{tennenbaum}.

Jarkko shows that linear and nonlinear dimensionality reduction methods can be visualized on separate datasets~\cite{dim-red-visual}, and visualization can provide an aid at analyzing which methods are better than others at finding an accurate lower representation of the data. The research paper~\cite{dimensionality-reduction-comparative-review} compares the performance of linear and nonlinear dimensionality reduction methods with some machine learning model.

According to Laurens, there is a tendency that the real world data is nonlinear. This means that the linear methods are at disadvantage, because they are not able to capture the intrinsic dimensionality of the nonlinear data, as good as nonlinear methods. However, he also states that nonlinear methods are not always able to outperform linear methods~\cite{dimensionality-reduction-comparative-review}, which might be seen as counterintuitive, since nonlinear methods are supposed to outperform linear methods on nonlinear data.


\subsection{Relevance of linear vs nonlinear methods}

%This project will focus on the dichotomy between linear and nonlinear methods and how they each affect the data. There will also be a focus on specifically the computational gains possible with these methods and how they handle different kinds of data.
%we want to explore whether these methods have a significant influence on the performance of a machine learning model~\cite{dimensionality-reduction-reddy,dimensionality-reduction-comparative-review}.
As outlined ealier, dimensionality reduction methods can be used to remove redundancy from data, which can improve the performance of a machine learning model. We have presented that nonlinear methods may work better on nonlinear datasets, and that they are suitable for nonlinear datasets, but according to Laurens~\cite{dimensionality-reduction-comparative-review}, there might not be a major difference between the linear and nonlinear methods. That is why we want to explore whether linear or nonlinear methods \textit{actually} have the most positive impact on our machine learning model.



% @techreport{sarwar2000application,
%   title={Application of dimensionality reduction in recommender system-a case study},
%   author={Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
%   year={2000},
%   institution={Minnesota Univ Minneapolis Dept of Computer Science}
% }


% In this section we have presented a short overview of the dimensionality reduction methods, i.e. linear and nonlinear methods. We have also presented some applications of the methods, and provided a reason for why we want to explore the methods in the project.


% @misc{dim-red-visual,
% title={{Dimensionality reduction for visual exploration of similarity structures}},
% author={Venna, Jarkko},
% year={2007},
% language={English},
% pages={81, [115]},
% publisher={Helsinki University of Technology},
% type={Doctoral thesis},
% keywords={dimensionality reduction, exploratory data analysis, information retrieval, information visualization, manifold learning, Markov Chain Monte Carlo},
% isbn={978-951-22-8752-9},
% series={Dissertations in computer and information science. Report D; 20},
% issn={1459-7020},
% url={}
% }

%@book{nonlinear-dim-red-chapter-one,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="High-Dimensional Data",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="1--16",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_1",
% url="https://doi.org/10.1007/978-0-387-39351-3_1"
% }

% @book{nonlinear-dim-red-chapter-two,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="Characteristics of an Analysis Method",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="17--45",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_2",
% url="https://doi.org/10.1007/978-0-387-39351-3_2"
% }

% @book{nonlinear-dim-red-chapter-three,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="Estimation of the Intrinsic Dimension",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="47--67",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_3",
% url="https://doi.org/10.1007/978-0-387-39351-3_3"
% }

