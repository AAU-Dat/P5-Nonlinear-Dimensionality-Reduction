\section{Linear versus nonlinear methods}\label{sec:linear-vs-nonlinear}

In this section we will explore dimensionality reduction more. More specifically, in this project we will distinguish between linear and nonlinear methods. According to Lee, there are several distinctions that can be made for dimensionality reduction methods. We will not focus on them, but will choose to classify them as linear and nonlinear because it is \textcquote{nonlinear-dim-red-chapter-two}{the straightest way to classify them}

The difference between linear and nonlinear methods is that linear methods express output as a linear combination of input data, and nonlinear methods do so in a nonlinear manner. This means that nonlinear methods are much more complex than the linear ones, and also more computationally expensive~\cite{nonlinear-dim-red-chapter-two}. The details regarding the difference between the methods will be presented in section~\ref{sec:dimensionality-reduction}.

\subsection{Applications of dimensionality reduction methods}
The use of the linear and nonlinear dimensionality reduction methods can among others be seen in~\cite{dimensionality-reduction-comparative-review, tennenbaum}, where the methods have been tested on artificial and real-world datasets. As an example, it has been shown that artificial datasets such as the swiss-roll show that linear methods fail to find the intrinsic dimensionality of the data as opposed to nonlinear methods~\cite{tennenbaum}.

Jarkko shows that linear and nonlinear dimensionality reduction methods can be visualized on separate datasets~\cite{dim-red-visual}, and visualization can provide an aid at analyzing which methods are better than others at finding an accurate lower representation of the data. The research paper~\cite{dimensionality-reduction-comparative-review} compares the performance of linear and nonlinear dimensionality reduction methods with some machine learning model.


According to Laurens, there is a tendency that the real world data is nonlinear. This means that the linear methods are at disadvantage, because they are not able to capture the intrinsic dimensionality of the nonlinear data as good as nonlinear methods. However, he also states that nonlinear methods are not always able to outperform linear methods~\cite{dimensionality-reduction-comparative-review}, which might be seen as counterintuitive, since nonlinear methods are supposed to outperform linear methods on nonlinear data.

\subsection{Relevance of linear vs nonlinear methods}
This project will focus this dichotomy between linear and non linear methods and how they each affect the data. There will also be a focus on specifically the computational gains possible with these methods and how they handle different kinds of data.
The reason why we focus on these things is we want to explore whether these methods have a significant influence on the performance of a machine learning model~\cite{dimensionality-reduction-reddy,dimensionality-reduction-comparative-review}. As outlined before, dimensionality reduction methods can be used to remove redundancy from data, which can improve the performance of a machine learning model. We have presented that nonlinear methods may work better on nonlinear datasets, and that they are suitable for nonlinear datasets, but according to Laurens~\cite{dimensionality-reduction-comparative-review}, there might not be a major difference between the linear and nonlinear methods. That is why we want to explore whether linear or nonlinear methods \textit{actually} have the most positive impact on our machine learning model.





% In this section we have presented a short overview of the dimensionality reduction methods, i.e. linear and nonlinear methods. We have also presented some applications of the methods, and provided a reason for why we want to explore the methods in the project.

% @misc{dim-red-visual,
% title={{Dimensionality reduction for visual exploration of similarity structures}},
% author={Venna, Jarkko},
% year={2007},
% language={English},
% pages={81, [115]},
% publisher={Helsinki University of Technology},
% type={Doctoral thesis},
% keywords={dimensionality reduction, exploratory data analysis, information retrieval, information visualization, manifold learning, Markov Chain Monte Carlo},
% isbn={978-951-22-8752-9},
% series={Dissertations in computer and information science. Report D; 20},
% issn={1459-7020},
% url={}
% }

%@book{nonlinear-dim-red-chapter-one,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="High-Dimensional Data",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="1--16",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_1",
% url="https://doi.org/10.1007/978-0-387-39351-3_1"
% }

% @book{nonlinear-dim-red-chapter-two,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="Characteristics of an Analysis Method",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="17--45",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_2",
% url="https://doi.org/10.1007/978-0-387-39351-3_2"
% }

% @book{nonlinear-dim-red-chapter-three,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="Estimation of the Intrinsic Dimension",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="47--67",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_3",
% url="https://doi.org/10.1007/978-0-387-39351-3_3"
% }

