\section{Linear versus nonlinear methods}\label{sec:linear-vs-nonlinear}
%keep the text if you want to use it
For this project the difference between dimensionality reduction techniques will solely be based on the aspect of linear and nonlinear models. According to Lee, there are several distinctions that can be made for dimensionality reduction methods, which will not be presented, but the distinction between linear and nonlinear methods is "the straightest way to classify them". The reason why the distinction is important lies in the methods' properties. Nonlinear methods are thought to be better than linear methods because the "connection between the latent variables and the observed ones may be much
richer than a simple matrix multiplication". On the other hand, nonlinear methods are computationally more expensive, and therefore may require more data. \cite{nonlinear-dim-red-chapter-two}


There is this book "Nonlinear Dimensionality Reduction" - I have put those refs \ref{nonlinear-dim-red-chapter-one}, and \ref{nonlinear-dim-red-chapter-two}. The first chapter of this book speaks about why DR is important. The book also contains a good list of contents.
%link to the book >> https://link.springer.com/content/pdf/10.1007/978-0-387-39351-3.pdf
Chapter one is good for some motivation(visualization, curse of dimensionality, manifold hypothesis), and chapter two goes into more detail about the DR methods(though not that important). The book is great - and it also goes into great detail concerning NLDR.


Another source which already writes about something about linear vs nonlinear is Laurens van der Maaten, where he does a comparative analysis of many DR methods, and concludes that PCA is the best method.


There is a ref \ref{dim-red-visual} that is a phd thesis on the visualization with linear/nonlinear methods. Talks about dim red but also a lot about visualization. The first chapters might prove to be useful. Great source.


% @misc{dim-red-visual,
% title={{Dimensionality reduction for visual exploration of similarity structures}},
% author={Venna, Jarkko},
% year={2007},
% language={English},
% pages={81, [115]},
% publisher={Helsinki University of Technology},
% type={Doctoral thesis},
% keywords={dimensionality reduction, exploratory data analysis, information retrieval, information visualization, manifold learning, Markov Chain Monte Carlo},
% isbn={978-951-22-8752-9},
% series={Dissertations in computer and information science. Report D; 20},
% issn={1459-7020},
% url={}
% }


%@book{nonlinear-dim-red-chapter-one,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="High-Dimensional Data",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="1--16",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_1",
% url="https://doi.org/10.1007/978-0-387-39351-3_1"
% }


% @book{nonlinear-dim-red-chapter-two,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="Characteristics of an Analysis Method",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="17--45",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_2",
% url="https://doi.org/10.1007/978-0-387-39351-3_2"
% }

