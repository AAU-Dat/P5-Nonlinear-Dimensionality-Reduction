\section{Linear versus nonlinear methods}\label{sec:linear-vs-nonlinear}

In this section \textbf{feature engeneering. Det er Sebastians sektion, som ikke er kommet ind endnu} we have seen some methods by which pre-processing can be achieved. In this section we will present a more specific family of feature engineering: dimensionality reduction. More specifically, in this project we will distinguish between linear and nonlinear methods. According to Lee, there are several distinctions that can be made for dimensionality reduction methods. We will not focus on them, but will choose to classify them as linear and nonlinear because it is "the straightest way to classify them" \cite{nonlinear-dim-red-chapter-two}.


The difference between linear and nonlinear methods is that linear methods express output as a linear combination of input data, and nonlinear methods do so in a nonlinear manner. This means that nonlinear methods are much more complex than the linear ones, but are also more computationally expensive \cite{nonlinear-dim-red-chapter-two}. The details regarding the difference between the methods will be presented in section \ref{sec:dimensionality-reduction}.

\subsection{Applications of dimensionality reduction methods}
The use of the linear and nonlinear dimensionality reduction methods can among others be seen in \cite{dimensionality-reduction-comparative-review} and \cite{tennenbaum}, where the methods have been tested on artificial and real-world datasets. As an example, it has been shown that artificial datasets such as the swiss-roll show that linear methods fail to find the intrinsic dimensionality of the data as opposed to nonlinear methods \cite{tennenbaum}. The research paper written by Jarkko shows that linear and nonlinear dimensionality reduction methods can be visualized on separate datasets \cite{dim-red-visual}, and visualization can provide an aid at analyzing which methods are better than others at finding an accurate lower representation of the data. The research paper \cite{dimensionality-reduction-comparative-review} compares the performance of linear and nonlinear dimensionality reduction methods with some machine learning model.


According to Laurens, there is a tendence that the real world data is nonlinear. This means that the linear methods are at disadvantage, because they are not able to capture the intrinsic dimensionality of the nonlinear data as good as nonlinear methods. However, he also states that nonlinear methods are not always able to outperform linear methods \cite{dimensionality-reduction-comparative-review}, which might be seen as counterintuitive, since nonlinear methods are supposed to outperform linear methods on nonlinear data.

\subsection{Relevance of linear vs nonlinear methods}
The reason why we focus on linear and nonlinear dimensionality reduction methods is because we want to explore whether these methods have a significant influence on the performance of a machine learning model. As outlined before, dimensionality reduction methods can be used to remove redundancy from data, which can improve the performance of a machine learning model. We have presented that nonlinear methods may work better on nonlinear datasets, and that they are suitable for nonlinear datasets, but according to Laurens \cite{dimensionality-reduction-comparative-review}, there might not be a major difference between the linear and nonlinear methods. That is is why we want to explore whether linear or nonlinear methods \textit{actually} have an impact on our machine learning model, based on the metrics that have been presented in this chapter. 


In this section we have presented a short overview of the dimensionality reduction methods, herunder linear and nonlinear methods. We have also presented some applications of the methods, and provided a reason for why we want to explore the methods in the project. \textbf{Det kan tages med eller slettes}: In this section we have also presented a potential challenge that we might face in the project: the fact that nonlinear methods may not be better than linear methods.

% @misc{dim-red-visual,
% title={{Dimensionality reduction for visual exploration of similarity structures}},
% author={Venna, Jarkko},
% year={2007},
% language={English},
% pages={81, [115]},
% publisher={Helsinki University of Technology},
% type={Doctoral thesis},
% keywords={dimensionality reduction, exploratory data analysis, information retrieval, information visualization, manifold learning, Markov Chain Monte Carlo},
% isbn={978-951-22-8752-9},
% series={Dissertations in computer and information science. Report D; 20},
% issn={1459-7020},
% url={}
% }

%@book{nonlinear-dim-red-chapter-one,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="High-Dimensional Data",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="1--16",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_1",
% url="https://doi.org/10.1007/978-0-387-39351-3_1"
% }

% @book{nonlinear-dim-red-chapter-two,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="Characteristics of an Analysis Method",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="17--45",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_2",
% url="https://doi.org/10.1007/978-0-387-39351-3_2"
% }

% @book{nonlinear-dim-red-chapter-three,
% author="Lee, John A.
% and Verleysen, Michel",
% editor="Lee, John A.
% and Verleysen, Michel",
% title="Estimation of the Intrinsic Dimension",
% bookTitle="Nonlinear Dimensionality Reduction",
% year="2007",
% publisher="Springer New York",
% address="New York, NY",
% pages="47--67",
% isbn="978-0-387-39351-3",
% doi="10.1007/978-0-387-39351-3_3",
% url="https://doi.org/10.1007/978-0-387-39351-3_3"
% }

