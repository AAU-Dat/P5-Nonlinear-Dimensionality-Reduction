\section{Linear versus nonlinear methods}\label{sec:linear-vs-nonlinear} \todo[inline]{Why is that classification important? and why not use Lees?}

In this section we will explore dimensionality reduction more. More specifically, in this project we will distinguish between linear and nonlinear methods. According to John A. Lee~\cite{nonlinear-dim-red-chapter-two}, there are several distinctions that can be made for dimensionality reduction methods. We will not focus on them, but will choose to classify them as linear and nonlinear because it is the most straightforward way of classifying them. Nonlinear methods are much more complex than the linear ones, and also more computationally expensive~\cite{nonlinear-dim-red-chapter-two}. The details regarding the difference between the methods will be presented in section~\ref{sec:dimensionality-reduction}.


\subsection{Results and differences of linear and nonlinear methods}
\todo[inline]{When do you define what linear and nonlinear are?According to this section are there good reasons to use nonlinear methods? write this more positive}
\todo[inline]{What does good and better mean? First when you apply linear, you can use it for visualization and FE. How are we going to evaluate for FE? Accuracy(model) perhaps}
\todo[inline]{The methods have different properties: Linear methods assume something called Linear independence of the features. One can of the data as having interactions; conceptually. When it is nonlinear, then nonexpected interactions can be applied. There are some interactions that are no longer interactions between those two variables.}

The linear and nonlinear dimensionality reduction methods can be seen in~\cite{dimensionality-reduction-comparative-review, tennenbaum}, where the methods have been tested on artificial and real-world datasets. As an example, it has shown that artificial datasets, such as the swiss-roll, show that linear methods have a more challenging time finding the intrinsic dimensionality of the data than nonlinear methods~\cite{tennenbaum}.

Jarkko shows that linear and nonlinear dimensionality reduction methods can be visualized on separate datasets~\cite{dim-red-visual}, and visualization can aid in analyzing which methods are better than others at finding an accurate lower representation of the data. The research paper~\cite{dimensionality-reduction-comparative-review} compares the performance of linear and nonlinear dimensionality reduction methods with some \gls{ml} models.

According to Laurens~\cite{dimensionality-reduction-comparative-review}, there is a tendency for real-world data to be nonlinear. The linear methods are at a disadvantage because they cannot capture the intrinsic dimensionality of the nonlinear data and nonlinear methods. However, he also states that nonlinear methods can only sometimes outperform linear methods~\cite{dimensionality-reduction-comparative-review}, which might be seen as counterintuitive since nonlinear methods are supposed to outperform linear methods on nonlinear data. 

As outlined earlier, dimensionality reduction methods can be used to remove redundancy from data, which can improve the performance of a \gls{ml} model. We have presented that nonlinear methods work better on nonlinear datasets and are suitable for nonlinear datasets. However, according to Laurens~\cite{dimensionality-reduction-comparative-review}, there might be a minor difference between the linear and nonlinear methods. We want to explore whether linear or nonlinear methods positively impact our \gls{ml} model. 

%This project will focus on the dichotomy between linear and nonlinear methods and how they each affect the data. There will also be a focus on specifically the computational gains possible with these methods and how they handle different kinds of data.
%we want to explore whether these methods have a significant influence on the performance of a machine learning model~\cite{dimensionality-reduction-reddy,dimensionality-reduction-comparative-review}.





% In this section we have presented a short overview of the dimensionality reduction methods, i.e. linear and nonlinear methods. We have also presented some applications of the methods, and provided a reason for why we want to explore the methods in the project.

