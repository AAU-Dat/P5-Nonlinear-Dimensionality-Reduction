% \section{Feature engineering}\label{sec:feature-engineering}
% % kort forklaring af FE, hvad er det 
% In machine learning \gls{fe} is used to transform the raw data into some type of data that is more suitable for the \gls{ml} model. A feature must derive from what type of data is given, and it is also tied to which model is being used. Some features are more appropriate for some types of models and vice versa. Feature engineering is the process of formulating the best fitted features given the task in hand, with the data and the chosen model~\cite{Feature-engineering-zheng}.

% There are many techniques for \gls{fe}, some examples on how to do \gls{fe} are: Imputation, handling outliers, scaling, dimensionality reduction. Imputation is the process of filling in missing values in the data. Most imputation is done by finding it by matrixes, by looking at other values in the dataset, a popular approach is k-nearest neighbors to find the missing values~\cite{imputation-for-tables-Biessmann}. Outliers are values that are far away from the rest of the data, and they can be a problem for some models, for both accuracy and inaccurate classification. It it therefore a good idea to eliminate the outliers, this is also a standard practice in most machine learning problems. There are many ways to handle outliers, one way is to remove them, another way is to replace them with the median or the mean of the data~\cite{outlier-perez}. 

% Scaling, also called feature normalization, is the process of transforming the data into a form that is more suitable for the \gls{ml} model. This is done by changing the range of the data, for example, if the data is in the range of 0-100, it can be scaled to be in the range of 0-1. This is done to make the data more suitable for the model, and to make it easier to compare the data. Scaling is also a standard practice in most machine learning problems. There are many ways to scale the data, one way is to use the min-max scaler, another way is to use variance scaling~\cite{Feature-engineering-zheng}.

% One of the goals of the dimensionality reduction methods is to counter the curse of dimensionality.
% According to Lee~\cite{nonlinear-dim-red-chapter-one}, the curse of dimensionality refers to "the number of data samples requried to estimate a function of several variables to a given accuracy on a given domain grows exponentially with the number of dimensions"~\cite{nonlinear-dim-red-chapter-one}. This means that machine learning models' performance might get affected by the huge amount of data that needs to be given.

% That is not optimal if we know that we can reduce the amount of data without losing too much information. On the contrary, the performance can perhaps be improved, partly because the size of the data gets reduced, and partly because the essence of the data is preserved. Dimensionality reduction will be further discussed in "chapter"~\ref{cha:theory}.


\section{Dimensionality reduction}\label{sec:dimensionality-reduction-problem}\todo[inline]{There are at least 2 ideas in this paragraph split it. Pointer.What is dimensionality reduction? What are its advantages? Computation? Interpretability? How does dimenstionality reduction work? This last question opens the gate to the section linear vs nonlinear}
Dimensionality is achieved by reducing the number of features(a feature is some measurable data) in a dataset. This can help to visualize the data or, within a large dataset, describe which data weighs heavier on the expected output of the \gls{ml} model. Time complexity is also a significant factor when discussing dimensionality reduction, as stated in "\textcquote{Analysis-of-Dimensionality-Reduction-Techniques-on-Big-Data}{Dimensionality reduction techniques can tremendously reduce the time complexity of training phase of ML algorithms hence reducing the burden of the machine learning algorithms.}. Time is a significant factor when discussing the \gls{ml} model, as the time complexity of the model is a significant factor in the overall time complexity of the system. Therefore, when reducing features in a dataset, it is crucial to find the features that are more relevant to the output of the model~\cite{Feature-engineering-zheng}. There are many ways to reduce the dimensionality of a dataset; the different types of dimensionality reduction will be further discussed in Chapter~\ref{cha:theory}.

There are many dimensionality reduction methods, and more are still being researched today~\cite{dimensionality-reduction-cheng}. Moreover, there are many different uses for these methods. An example of this could be to improve heuristic models for explaining the data from surveys better, through better visualizations as it helps to improve understanding~\cite{dimensionality-reduction-cheng}. Many more examples exist, but the main point is that dimensionality reduction is a handy tool in many different fields and a vital part of the \gls{ml} model. We will categorize dimensionality reduction into two general categories in the following section: Linear and nonlinear dimensionality reduction. 
% Forklar hvorfor vi har valgt at fokusere p√• dimensionalitets reduction 
