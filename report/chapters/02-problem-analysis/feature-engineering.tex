% \section{Feature engineering}\label{sec:feature-engineering}
% % kort forklaring af FE, hvad er det 
% In machine learning \gls{fe} is used to transform the raw data into some type of data that is more suitable for the \gls{ml} model. A feature must derive from what type of data is given, and it is also tied to which model is being used. Some features are more appropriate for some types of models and vice versa. Feature engineering is the process of formulating the best fitted features given the task in hand, with the data and the chosen model~\cite{Feature-engineering-zheng}.

% There are many techniques for \gls{fe}, some examples on how to do \gls{fe} are: Imputation, handling outliers, scaling, dimensionality reduction. Imputation is the process of filling in missing values in the data. Most imputation is done by finding it by matrixes, by looking at other values in the dataset, a popular approach is k-nearest neighbors to find the missing values~\cite{imputation-for-tables-Biessmann}. Outliers are values that are far away from the rest of the data, and they can be a problem for some models, for both accuracy and inaccurate classification. It it therefore a good idea to eliminate the outliers, this is also a standard practice in most machine learning problems. There are many ways to handle outliers, one way is to remove them, another way is to replace them with the median or the mean of the data~\cite{outlier-perez}. 

% Scaling, also called feature normalization, is the process of transforming the data into a form that is more suitable for the \gls{ml} model. This is done by changing the range of the data, for example, if the data is in the range of 0-100, it can be scaled to be in the range of 0-1. This is done to make the data more suitable for the model, and to make it easier to compare the data. Scaling is also a standard practice in most machine learning problems. There are many ways to scale the data, one way is to use the min-max scaler, another way is to use variance scaling~\cite{Feature-engineering-zheng}.

% One of the goals of the dimensionality reduction methods is to counter the curse of dimensionality.
% According to Lee~\cite{nonlinear-dim-red-chapter-one}, the curse of dimensionality refers to "the number of data samples requried to estimate a function of several variables to a given accuracy on a given domain grows exponentially with the number of dimensions"~\cite{nonlinear-dim-red-chapter-one}. This means that machine learning models' performance might get affected by the huge amount of data that needs to be given.

% That is not optimal if we know that we can reduce the amount of data without losing too much information. On the contrary, the performance can perhaps be improved, partly because the size of the data gets reduced, and partly because the essence of the data is preserved. Dimensionality reduction will be further discussed in chapter~\ref{cha:theory}.


\section{Dimensionality reduction}\label{sec:dimensionality-reduction-problem}
Dimensionality is achieved by reducing the number of features(a feature is some measurable data) in a dataset. This can help to visualize the data, or within a large dataset, describe which data weighs heavier on the expected output of the \gls{ml} model. Time complexity is also a large factor when discussing dimensionality reduction, as stated in \cite{Analysis-of-Dimensionality-Reduction-Techniques-on-Big-Data}: "Dimensionality reduction techniques can tremendously reduce the time complexity of training phase of ML algorithms hence reducing the burden of the machine learning algorithms.". This is a large factor when discussing the \gls{ml} model, as the time complexity of the model is a large factor in the overall time complexity of the system. Therefore, when reducing features in a dataset it is important to find the features that are more relevant to the output of the model~\cite{Feature-engineering-zheng}. There are many ways to reduce the dimensionality of a dataset, the different types of dimensionality reduction will be further discussed in Chapter~\ref{cha:theory}.

There are many dimensionality reduction methods and more are still being researched today~\cite{dimensionality-reduction-cheng}. And there are many different uses for these methods. An example of this, could be to improve heuristic models for explaining the data from surveys better, through better visualizations as it helps to improve understanding~\cite{dimensionality-reduction-cheng}. Many more examples exist, but the main point is that dimensionality reduction is a very useful tool in many different fields, and it is a very important part of the \gls{ml} model. We will categorize dimensionality reduction into two general categories in the following section, and discuss them in further detail. Linear and nonlinear dimensionality reduction.


% @ARTICLE{Analysis-of-Dimensionality-Reduction-Techniques-on-Big-Data,
% author={Reddy, G. Thippa and Reddy, M. Praveen Kumar and Lakshmanna, Kuruva and Kaluri, Rajesh and Rajput, Dharmendra Singh and Srivastava, Gautam and Baker, Thar},
% journal={IEEE Access}, 
% title={Analysis of Dimensionality Reduction Techniques on Big Data}, 
% year={2020},
% volume={8},
% number={},
% pages={54776-54788},
% doi={10.1109/ACCESS.2020.2980942}}



% Forklar hvorfor vi har valgt at fokusere p√• dimensionalitets reduction 


% @book{Feature-engineering-zheng,
% author = {Zheng, Alice and Casari, Amanda},
% title = {Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists},
% year = {2018},
% isbn = {1491953241},
% }

% @article{imputation-for-tables-Biessmann,
%   author  = {Felix Biessmann and Tammo Rukat and Phillipp Schmidt and Prathik Naidu and Sebastian Schelter and Andrey Taptunov and Dustin Lange and David Salinas},
%   title   = {DataWig: Missing Value Imputation for Tables},
%   year    = {2019},
%   url     = {http://jmlr.org/papers/v20/18-753.html}
% }

% @article{outlier-perez,
% author = {Perez, Husein and Tah, Joseph H. M.},
% title = {Improving the Accuracy of Convolutional Neural Networks by Identifying and Removing Outlier Images in Datasets Using t-SNE},
% year = {2020},
% url = {https://www.mdpi.com/2227-7390/8/5/662},
% ISSN = {2227-7390},
% DOI = {10.3390/math8050662}
% }

%@book{nonlinear-dim-red-chapter-one,
% author={Lee, John A. and Verleysen, Michel},
% title={High-Dimensional Data},
% bookTitle={Nonlinear Dimensionality Reduction},
% year={2007},
% isbn={978-0-387-39351-3},
% doi={10.1007/978-0-387-39351-3_1},
% url={https://doi.org/10.1007/978-0-387-39351-3_1}
% }


% @article{dimensionality-reduction-reddy,
%   author={Reddy, G. Thippa and Reddy, M. Praveen Kumar and Lakshmanna, Kuruva and Kaluri, Rajesh and Rajput, Dharmendra Singh and Srivastava, Gautam and Baker, Thar},
%   journal={IEEE Access}, 
%   title={Analysis of Dimensionality Reduction Techniques on Big Data}, 
%   year={2020},
%   doi={10.1109/ACCESS.2020.2980942}}

% @article{dimensionality-reduction-cheng,
% author = {Cheng, Zhun and Lu, Zhixiong},
% year = {2018},
% title = {A Novel Efficient Feature Dimensionality Reduction Method and Its Application in Engineering},
% journal = {Complexity},
% doi = {10.1155/2018/2879640}
% }

% @article{dimensionality-reduction-maitra,
%   author={Maitra, Shithi and Hossain, Tonmoy and Hasib, Khan Md. and Shishir, Fairuz Shadmani},
%   title={Graph Theory for Dimensionality Reduction: A Case Study to Prognosticate Parkinson's}, 
%   year={2020},
%   doi={10.1109/IEMCON51383.2020.9284926}}