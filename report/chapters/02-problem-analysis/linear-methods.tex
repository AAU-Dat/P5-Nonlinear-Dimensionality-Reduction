\section*{Linear methods}
This section will present the chosen linear methods that will be used in the project. The reason why linear methods were chosen is because they are a relevant for the analysis of the high-dimensional nature of image data.\cite{Cunningham}

\begin{comment}
@misc{Cunningham,
      organization = {Journal of Machine Learning Research},
      url          = {https://stat.columbia.edu/~cunningham/pdf/CunninghamJMLR2015.pdf},
      title        = {Linear Dimensionality Reduction:Survey, Insights, and Generalizations},
      author       = {John P. Cunningham, Zoubin Ghahramani},
      urldate      = {2022-10-11}
    }
\end{comment}


\subsection*{Principal Components Analysis(PCA)}
PCA tries to map the high-dimensional data so that it maximizes the amount of variance in the embedded data. By maximizing the variance it can be expressed as maximizing the cost function

C($\mathbf{Y}$) $tr(M \top cov(X)M)$, where cov($\mathbf{X}$) is the covariance matrix for $\mathbf{X}$. The covariance matrix for the embedded data $S_y$ can be calculated as

$S_y = \frac{1}{N}\sum\limits_{i=1}^{N}(y_i - \mu)(y_i - \mu)\top$, where $\mu$ denotes the average value for each column

$\mu = \frac{1}{N}\sum\limits_{i=1}^{N}x_i$ 

Hence, after calculating the covariance matrix for $\mathbf{X}$, it can form the linear mapping by applying eigendecomposition on

$cov(X)M = \lambda M$, where

$\lambda$ represents eigenvalues with $d$ eigenvectors(also called principal components), which are chosen. Thus mapping the high-dimensional data onto lower dimensions can be written as $\mathbf{Y=XM}$

It should be noted, that some of the linear and non-linear methods usually try to solve an optimization problem, in which the function has the form of $\phi$(Y) = $\frac{Y\top AY}{Y\top BY}$, which can be optimized by using eigendecomposition.\cite{LVD}

\begin{comment}
    @misc{LVD,
          organization = {Tilburg centre for Creative Computing},
          url          = {https://lvdmaaten.github.io/publications/papers/TR\_Dimensionality\_Reduction\_Review\_2009.pdf},
          title        = {Dimensionality Reduction: A Comparative Review},
          author       = {Laurens van der Maaten, Eric Postma, Jaap van Henrik},
          urldate      = {2009-10-26}
        }
\end{comment}
    

\subsection*{Factor Analysis (FA)}
FA tries to correlate the input variables into fewer latent factors, or variables, where each variable supplies parto fht weight of the factor. It can be noted that the relation between the FA family and PCA is that PCA is a type of FA, specifically Exploratory Factor Analysis (EFA). EFA contains a type, called Principal Factor Analysis (PFA).

The difference between the PFA and PCA is that PFA assume that there are unobserved data that need to be be extracted, while PCA extracts meaning from the data itself.\cite{FA}


\begin{comment}
@misc{FA,
      organization = {Advanced Research Computing Statistical Methods and Data Analytics},
      url          = {https://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/},
      title        = {A PRACTICAL INTRODUCTION TO FACTOR ANALYSIS: EXPLORATORY FACTOR ANALYSIS},
      urldate      = {2022-10-11}
    }
\end{comment}

\subsection*{Non-negative matrix factorization (NMF)}
NMV tries to factorize the input matrix $X$ into two matrices $WH$ such that $WH ~ A$. This means that NMF tries to minimize the difference between the $X$ and $WH$. \cite{NMF}

\begin{comment}
@misc{NMF,
  organization = {the morning paper},
  url          = {https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/},
  title        = {The why and how of nonnegative matrix factorization},
  author       = {Adrian Colyer},
  urldate      = {2019-02-18}
}
\end{comment} 

\subsection*{Linear Discriminant Analysis (LDA)}
LDA is trying construct a lower dimensional space which maximizes the ratio between \textbf{between-class} variance and \textbf{within-class} variance, which maximizes class separability. By between-class it is meant the distance between means of different classes, and within-class as the distance between the mean and the observations of each class. In one equation, $LDA = \frac{Variance_{BetweenClass}}{Variance_{WithinClass}}$ LDA maximizes the separability between classes and minimizes the separability within classes so as to get the largest value possible.

\begin{comment}
@misc{LDA2,
      organization = {University of Salford Manchester},
      url          = {http://usir.salford.ac.uk/id/eprint/52074/1/AI\_Com\_LDA\_Tarek.pdf},
      title        = {Linear discriminant analysis: a detailed tutorial},
      urldate      = {2022-10-11}
    }
    \end{comment} 
    
LDA can be divided into two types: class-independent (CI) and class-dependent (CD). CI computes one global transformation matrix for the data, where as CD takes a more local approach; it computes the transformation matrix for each class, along with the eigenvectors and eigenvalues.\cite{LDA2}

