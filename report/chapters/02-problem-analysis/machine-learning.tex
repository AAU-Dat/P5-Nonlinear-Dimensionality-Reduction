\section{Machine learning}\label{sec:machine-learning}
Figure~\ref{fig:basic-machine-learning-pipeline} illustrates a general \gls{ml} pipeline.

To compare and contrast linear and nonlinear methods through the lens of a model Efficiently the use of a pipeline is needed. A brief Introduction to pipelines, there use and how to introduce dimensionality reduction to them the purpose of this section.

A pipeline generally consists of four main steps: data, \gls{fe}, \gls{ml} model training, and model evaluation. First stem is the data step which essentially is the collection of data. The \gls{fe} step is used to transform the data into a form that is more suitable for the \gls{ml} model. The \gls{ml} model training step is used to train the \gls{ml} model on the data. The model evaluation step is used to evaluate the performance of the \gls{ml} model. The \gls{ml} model can then be used to make predictions on new data. \todo[inline]{Add text on the data step}


\begin{figure}[htb!]
    \centering
    \begin{tikzpicture}
        \node (b) [state] {feature engineering};
        \node (c) [state, shift={($(b.east)+(2cm,0)$)}] {model};
        \node (a) [state, shift={($(b.west)+(-2cm,0)$)}] {data};
        \node (d) [state, shift={($(c.east)+(2cm,0)$)}] {evaluation};
        \node (e) [state, shift={($(b.south)+(0,-2cm)$)}] {parameters};

        \draw[arrow, ->] (a) -- node[above,scale=.70,align=center,] {} (b);
        \draw[arrow, ->] (b) -- node[above,scale=.70,align=center,] {} (c);
        \draw[arrow, ->] (c) -- node[above,scale=.70,align=center,] {} (d);
        \draw[arrow, ->] (e) -- node[above,scale=.70,align=center,] {} (c);

        \draw[arrow, ->] (d.north) -- ++(0,0.75) -| (b);
        \draw[arrow, ->] (d.south) -- ++(0,-0.75) -| (c);
    \end{tikzpicture}
    \caption{Simplified machine learning pipeline}
    \label{fig:basic-machine-learning-pipeline}
\end{figure}

The \gls{fe} step is where Dimmensionality reduction will be relevant. This is where the dimmensions are reduced to increase perfomance of the model. This and the model is then evaluated to see if they can be tuned for better results. This tuning is generraly refered to as hyperparameter tuning. What the best hyper parameter tuning is depends on the data. This leads to the last decision before the problem statement, The choice of data.

\section{Data}\label{sec:data}
We have chosen to work with \gls{mnist}, which is descibed by its creators as
\textcquote{lecun-mnist-database}{The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image}.


The reason why \gls{mnist} was chosen is because it has real-world uses inside of image recognition, particularly the assignment of recognizing handwritten numbers. The real-world use comes from detecting patterns in which numbers are written, which can be attributed due to the style of the writer, and / or artificial errors.


Another reason is because there are many projects, which revolve around recognizing handwritten digits using machine learning. That means that we can compare our results to other similar projects for better discussion of results. On a similar vein, the digits in the datset are \textcquote{lecun-mnist-database}{size-normalized and centered in a fixed-size image}, which means that Lecun et al.\ have worked with the data. The project could also have gone another route: collecting data, cleaning data, etc.\,but choosing the \gls{mnist} dataset has simplified the overall work for us, and allowing to focus more on the machine learning / dimensionality reduction part of the project. 


The group has also considered the Iris dataset, another introductory dataset, however that dataset might have too few data samples ~\cite{mnist-vs-iris}, which is also why the group has chosen to work with \gls{mnist}. As outlined before, there have been done some sort of preprocessing regarding the \gls{mnist}, which further solidified the choice regarding \gls{mnist}.


We have in this chapter presented the pipeline, and gave an overview of the different components of the pipeline. With this knowledege we can now move on to the problem statement.

%@misc{mnist-vs-iris,
% author       = {Matteo Kimura},
% url          = {https://lamfo-unb.github.io/2019/05/17/Introductory-Datasets/},
% title        = {Introductory Datasets},
% urldate      = {2022-11-15}
% }