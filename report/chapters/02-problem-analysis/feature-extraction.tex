\section{Feature extraction}\label{sec:feature-extraction}
% kort forklaring af FE, hvad er det 
In machine learning \gls{fe} is used to transform the raw data into some type of data that is more suitable for the \gls{ml} model, this is why a feature is not a specific method, can no feature looks the same. A feature must derive from what type of data is given, and it is also tied to which model is being used. Some features are more appropriate for some types of models and vice versa. Feature engineering is the process of formulating the best fitted features given the task in hand, with the data and the chosen model~\cite{Feature-engineering-zheng}. 
% hvilke sub kategorier indgår i FE
There are many techniques for \gls{fe}, some examples on how to do \gls{fe} are: Imputation, handling outliers, scaling, dimensionality reduction. Imputation is the process of filling in missing values in the data, most imputation is done by finding it by matrixes, by looking at other values in the dataset, a popular approach is k-nearest neighbors to find the missing values~\cite{imputation-for-tables-Biessmann}. Outliers are values that are far away from the rest of the data, and they can be a problem for some models, for both accuracy and inaccurate classification. It it therefore a good idea to eliminate the outliers, this is also a standard practice in most machine learning problems. There are many ways to handle outliers, one way is to remove them, another way is to replace them with the median or the mean of the data~\cite{outlier-perez}. Scaling, also called feature normalization, is the process of transforming the data into a form that is more suitable for the \gls{ml} model. This is done by changing the range of the data, for example, if the data is in the range of 0-100, it can be scaled to be in the range of 0-1. This is done to make the data more suitable for the model, and to make it easier to compare the data. Scaling is also a standard practice in most machine learning problems. There are many ways to scale the data, one way is to use the min-max scaler, another way is to use variance scaling~\cite{Feature-engineering-zheng}. 

The goal of the dimensionality reduction methods is to reduce the curse of dimensionality. 
Dimensionality reduction is the process of . This is done by reducing the number of features in a dataset. This for example can help to visioalize the data, or within a large dataset, describe which data weighs heavier on the expected output of the \gls{ml} model. Therefore when reducing features in a dataset it is important to find the features that are more relevant to the output of the model. There are many ways to reduce the dimensionality of a dataset, the different types of dimensionality reduction will be further discussed in chapter~\ref{cha:theory}~\cite{Feature-engineering-zheng}. 

We have chosen to work with dimensionality reduction methods, because there are many different dimensionality reduction methods and is still being researched today, 
% Hvorfor er det spændende, find kilder hvor de gør noget spændende 
for example, new methods are still being created today for specific \gls{ml} models, as seen in~\cite{dimensionality-reduction-cheng}. Where they try to make method to improve a heuristic model for explaining the data from surveys better, for surveys a good metric would be how to better visionize the data in diagrams. When discussing which dimensionality reduction method is best it is important to consider what is most important for the method, in~\cite{dimensionality-reduction-maitra} they investegate which method is best for computational graph theory, using known methods in \gls{pca} and \gls{lda} for finding the best model. In~\cite{dimensionality-reduction-reddy} they compare \gls{pca} and \gls{lda} on which is best on the dataset Cardiotocography with different \gls{ml} methods, and finds out \gls{pca} outperformance \gls{lda} when experimenting results at the stat. In~\cite{dimensionality-reduction-comparative-review} they compare \gls{pca} with diffrent nonlinear dimensionality reduction methods with artificial and natural datasets. They find out that nonlinear methods perform well on arificial datasets, but not on natural datasets, as \gls{pca} does. 
According to Lee~\cite{nonlinear-dim-red-chapter-one}, the curse of dimensionality refers to "the number of data samples requried to estimate a function of several variables to a given accuracy on a given domain grows exponentially with the number of dimensions"~\cite{nonlinear-dim-red-chapter-one}. This means that machine learning models' performance might get affected by the huge amount of data that needs to be given.

That is not optimal if we know that we can reduce the amount of data without losing too much information. On the contrary, the performance can perhaps be improved, partly because the size of the data gets reduced, and partly because the essence of the data is preserved. Dimensionality reduction will be further discussed in chapter~\ref{cha:theory}.  

% Forklar hvorfor vi har valgt at fokusere på dimensionalitets reduction 


% @book{Feature-engineering-zheng,
% author = {Zheng, Alice and Casari, Amanda},
% title = {Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists},
% year = {2018},
% isbn = {1491953241},
% }

% @article{imputation-for-tables-Biessmann,
%   author  = {Felix Biessmann and Tammo Rukat and Phillipp Schmidt and Prathik Naidu and Sebastian Schelter and Andrey Taptunov and Dustin Lange and David Salinas},
%   title   = {DataWig: Missing Value Imputation for Tables},
%   year    = {2019},
%   url     = {http://jmlr.org/papers/v20/18-753.html}
% }

% @article{outlier-perez,
% author = {Perez, Husein and Tah, Joseph H. M.},
% title = {Improving the Accuracy of Convolutional Neural Networks by Identifying and Removing Outlier Images in Datasets Using t-SNE},
% year = {2020},
% url = {https://www.mdpi.com/2227-7390/8/5/662},
% ISSN = {2227-7390},
% DOI = {10.3390/math8050662}
% }

%@book{nonlinear-dim-red-chapter-one,
% author={Lee, John A. and Verleysen, Michel},
% title={High-Dimensional Data},
% bookTitle={Nonlinear Dimensionality Reduction},
% year={2007},
% isbn={978-0-387-39351-3},
% doi={10.1007/978-0-387-39351-3_1},
% url={https://doi.org/10.1007/978-0-387-39351-3_1}
% }


% @article{dimensionality-reduction-reddy,
%   author={Reddy, G. Thippa and Reddy, M. Praveen Kumar and Lakshmanna, Kuruva and Kaluri, Rajesh and Rajput, Dharmendra Singh and Srivastava, Gautam and Baker, Thar},
%   journal={IEEE Access}, 
%   title={Analysis of Dimensionality Reduction Techniques on Big Data}, 
%   year={2020},
%   doi={10.1109/ACCESS.2020.2980942}}

% @article{dimensionality-reduction-cheng,
% author = {Cheng, Zhun and Lu, Zhixiong},
% year = {2018},
% title = {A Novel Efficient Feature Dimensionality Reduction Method and Its Application in Engineering},
% journal = {Complexity},
% doi = {10.1155/2018/2879640}
% }

% @article{dimensionality-reduction-maitra,
%   author={Maitra, Shithi and Hossain, Tonmoy and Hasib, Khan Md. and Shishir, Fairuz Shadmani},
%   title={Graph Theory for Dimensionality Reduction: A Case Study to Prognosticate Parkinson's}, 
%   year={2020},
%   doi={10.1109/IEMCON51383.2020.9284926}}