\section{Used methods in MNIST}
\label{sec:method-MNIST}
When comparing the results of projects using the MNIST dataset, it is important to know which feature engineering and classification methods were used. In this section, we will describe the methods used in the projects that we have analyzed. 
On the MNIST webcite \cite{MNIST} there is a tabel of all the official methods used on the MNIST dataset with the last posted in 2012. As this project is based on the MNIST data set, the methods used in this project will be compared to some of the methods used on the MNIST dataset. One thing to note, when looking in the table on the MNIST webcite, is there is a column that describes the preprocessing of the data. 
This is not the same as the feature engineering, which is the process of extracting features from the data. The preprocessing is the process of cleaning the data, which is done before the feature engineering. As this project focuses on feature engineering, it is important what feature engineering that has been used on the projects when compairng it to this projects result. 
\begin{table}[h]
  \centering
  \resizebox{1\textwidth}{!}
  {
  \begin{tabular}{|c|c|c|}
      \hline
      Classifier & Preprocessing & Test error rate(\%)\\
      \hline
      \multicolumn{3}{|c|}{\textbf{Linear classifiers}} \\
      \hline
      linear classifier(1-layer NN) & none & 12.0 \\
      linear classifier(1-layer NN) & deskewing & 8.4 \\
      pairwise linear classifier & deskewing & 7.6 \\
      \hline
      ... & ... & ... \\ 
      \hline
      \multicolumn{3}{|c|}{\textbf{Non-linear classifiers}} \\
      \hline
      40 PCA + quadratic classifier & none & 3.3 \\
      1000 RBF + linear classifier & none & 3.6 \\
      \hline
      ... & ... & ... \\ 
      \hline
      \multicolumn{3}{|c|}{\textbf{Convolutional nets}} \\
      \hline
      committee of 35 conv. net, 1-20-P-40-P-150-10 [elastic distortions] & width normalization & 0.23 \\
      \hline
  \end{tabular}
  }
  \caption{The methods previusly used on the MNIST data set}
  \label{tab:method-MNIST}
\end{table}
There are not many projects that use feature engineering on the MNIST dataset as seen here \cite{Stochastic-optimization-neural-networks-assiri, BYERLY2021545, convolutional-neural-networks-convnets, Multi-column-neural-network-ciregan, WaveMix-jeevan}. These only use a specific machine learning model, with the raw data from MNIST, so there is no feature engineering on them. Therefore we can not fully compare our results, as the projects are researching different models to get the best outcome where this project is researching different feature engineering methods to get the best outcome. Some of the results from the projects are shown in Table \ref{tab:method-MNIST}.
In Table \ref{tab:method-MNIST} there is the linear classifiers, specificly the one with none preprocessed data, that has an error rate of 12.0\% which would be the one closest to this project. 

In the Table \ref{tab:method-MNIST} there is also the non-linear classifiers, both of them has not have any preprocessed data, which both could be compared to the result this projects model will get. The first one is the PCA + quadratic classifier with an error rate of 3.3\% and the second one is the RPF + linear classifier with an error rate of 3.6\%. 
On the MNIST webcite \cite{MNIST} the project with the lowest error procentage is the Committee of 35 conv. net, 1-20-P-40-P-150-10 [elastic distortions] by Dan Cireşan, Ueli Meier and Juergen Schmidhuberthe, this was done with preprocessed data by width normalization and a error rate of 0.23\%.

@misc{MNIST,
  author = {Yann LeCun, Corinna Cortes, Christopher J.C. Burges},
  url          = {http://yann.lecun.com/exdb/mnist/},
  title        = {THE MNIST DATABASE},
  urldate      = {2022-10-06}
}
@misc{Stochastic-optimization-neural-networks-assiri,
  author = {Yahia Saeed Assiri},
  url          = {https://arxiv.org/ftp/arxiv/papers/2001/2001.08856.pdf},
  title        = {Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods},
  urldate      = {2022-10-11}
}
@article{BYERLY2021545,
title = {No routing needed between capsules},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.08.064},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221012546},
author = {Adam Byerly and Tatiana Kalganova and Ian Dear},
}
@misc{convolutional-neural-networks-convnets,
  doi = {10.48550/ARXIV.2008.10400},
  url = {https://arxiv.org/abs/2008.10400},
  author = {An, Sanghyeon and Lee, Minjun and Park, Sanglee and Yang, Heerin and So, Jungmin},
  title = {An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit Recognition},
  year = {2020},
}
@INPROCEEDINGS{Multi-column-neural-network-ciregan,  
author={Ciregan, Dan and Meier, Ueli and Schmidhuber, Jürgen},    
title={Multi-column deep neural networks for image classification},   
year={2012},  
doi={10.1109/CVPR.2012.6248110}
}
@misc{WaveMix-jeevan,
  doi = {10.48550/ARXIV.2205.14375},
  url = {https://arxiv.org/abs/2205.14375},
  author = {Jeevan, Pranav and Viswanathan, Kavitha and Sethi, Amit},
  title = {WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis},
  year = {2022},
  }