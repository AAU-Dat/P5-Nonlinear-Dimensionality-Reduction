\tikzstyle{circle} = [ellipse, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=white!30]
\section{Python pipeline}\label{sec:python-pipeline}
The python pipeline is based on the model in Figure~\ref{fig:python-pipeline-model}. The pipeline is divided into five tasks, which are described in the following sections. 

\begin{figure}[htb!]
    \centering
    \input{figures/python-pipeline.tex}
    \caption{Python pipeline}
    \label{fig:Python-pipeline}
\end{figure}

An overview of the pipeline can be seen in Figure~\ref{fig:Python-pipeline}. The pipeline is divided into \textbf{six} tasks, which are described in the following sections. 

The first task is pre-preprocessing the data.  Pre-preprocessing the data includes rotating pixels from the data, which should make it harder for the model to learn. The step can also be added so as to research whether there will be a difference in the dimensionality reduction methods with augmented data. The second task is loading the \gls{mnist} data set. The data set is divided into three subsets: the training, validation, and test sets.


The third task is the cross-validation task that repeats itself. The reason it repeats itself is that it evaluates the respective model with the respective dimensionality reduction method using k-fold. The amount of repetitions made depends on the number of folds selected and the number of chosen hyperparameters. Essentially, every fold contains all of the possible combinations of hyperparameters for the dimensionality reduction method and machine learning model, but on different segments of data, where a pipeline, as shown in Figure \ref{fig:python-individual-pipeline} is run.

\begin{figure}[htb!]
    \centering
    \input{figures/python-pipeline.tex}
    \caption{Python pipeline}
    \label{fig:python-individual-pipeline}
\end{figure}

In the pipeline for an individual combination of hyperparameters, the data is pre-processed. Pre-processing is done by rescaling the image or making the image only black and white because the chosen data set can be the augmented set, which already has some noise in the image. This task can be optional, making it harder for the model to learn the data. Dimensionality reduction is then performed on the data, and can be omitted if evaluation of the base model needs to be performed. The model is then trained on the data, and the model is evaluated on the validation set. Output metrics will be such as the time and accuracy of the model for the respective fold will be considered. These metrics will be used so compare the effect of hyperparameters on the dimensionality reduction methods and machine learning models.


Returning back to the pipeline presented in Figure \ref{fig:Python-pipeline}, the fourth task is training the with the best hyperparameters selected with regards to the respective dimensionality reduction method and model.

The fifth task is evaluating the model on the test set. The evaluation is done with respect to the metrics explainability, accuracy, precision, recall, f1-score, speed/run time, \textbf{and memory usage} \todo{We do not do that... do we?}, which will be presented in a format that can be used for comparison.

\begin{figure}[htb!]
    \centering
    \input{figures/data-argumentation.tex}
    \caption{Data argumentation creation}
    \label{fig:data-argumentation-create}
\end{figure}


\todo{I do not know if we need what is under this todo note}
In Figure~\ref{fig:data-argumentation-create} data argumentation is visualized. The pipeline is divided into three tasks. The first task is data augmentation, where it is decided what type of augmentation is used. This can be a blur, rotation, noise, or a combination of these. The second task is then to use the augmentation on the MNIST data set, where the output is the augmented MNIST data set. The new set will be the input to the machine learning model. It should be noted that the size of the augmented data set can be bigger than the original MNIST data set.

\begin{figure}[htb!]
    \centering
    \input{figures/data-separation.tex}
    \caption{Data-set separation}
    \label{fig:data-set-sepa}
\end{figure}

In Figure~\ref{fig:data-set-sepa} describes how the data-set from MNIST will be devided. It will be devided into two data-sets, one for training and one for testing. The training data-set will be used to train the machine learning model. The default size is 80\% for training and 20\% for testing.