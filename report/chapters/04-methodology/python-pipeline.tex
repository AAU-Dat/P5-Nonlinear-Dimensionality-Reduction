\tikzstyle{circle} = [ellipse, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=white!30]
\section{Python pipeline}\label{sec:python-pipeline}
The python pipeline is based on the model in Figure~\ref{fig:python-pipeline-model}. The pipeline is divided into 5 tasks, which are described in the following sections. 

\begin{figure}[htb!]
    \centering
    \input{figures/python-pipeline.tex}
    \caption{Python pipeline}
    \label{fig:Python-pipeline}
\end{figure}

An overview of the pipeline can be seen in Figure~\ref{fig:Python-pipeline}. The pipeline is divided into 5 tasks, which are described in the following sections. 

 
The first task is loading the \gls{mnist} data set. The data set is divided into three subsets: the training, validation, and test sets. \todo{the data is not divided right now. I will be need to be divided.}

The reason why there are two image files depicted is because the other data set can be made in the pre-processing task. \todo{unclear whether we will have augmentation}


The second task is pre-processing the data. This is done by rescaling the image or making the image only black and white because the chosen data set can be the augmented set, which already has some noise in the image. This task can be optional to make it harder for the model to learn the data.

#The model trains with the training set, tunes the hyperparameters with the validation set and evaluates the test set.
The third task is dimensionality reduction, where the data's dimensions can be reduced. The motive for not reducing the data is the need to have the model's base metrics without any dimensionality reduction; in that way, a comparison between the base model and model with the different dimensionality reduction methods regarding their results can be presented.

The fourth task is the machine learning model, which trains the model with the chosen reduced or original train data set depending on previous choices. In order to not overfit the machine learning model, cross-validation will be implemented. Cross-validation can be seen as an abstraction of he tasks two to four. In essence, the data gets passed to cross-validation, which leads to the machine learning model with the best hyperparameters. The model is evaluated on the test set, and the results are presented in the fifth task.


The model trains with the training set and validation set, and the test results are then evaluated and compared to the results of the other models. The evaluation is done by explainability, accuracy, precision, recall, f1-score, speed/run time, and memory usage, which will presented in a format that can be used for comparison.


Cross-validation gives the possibility to tune the hyperparameters of the methods and models. In the project, the implementation of cross-validation is done by using GridSearchCV, which further can output the scores of all the invididual models trained with the different hyperparameters.


Depending on how many folds there are made, for each fold a number $n$ of subtasks will be performed. The number of subtasks corresponds to the Cartesian product of the hyperparameters. More on that in Section \textbf{I do no have ref}. A subtask is defined as a fit of the pipeline object that contains the steps of the pipeline; pre-processing, dimensionality reduction, model training and evaluation(on the validation set). Each subtask has the hyperparameters set to a specific value, which belongs to the Cartesian product of the hyperparameters.


\begin{figure}[htb!]
    \centering
    \input{figures/data-argumentation.tex}
    \caption{Data argumentation creation}
    \label{fig:data-argumentation-create}
\end{figure}

In Figure~\ref{fig:data-argumentation-create} the data argumentation is visualized. The pipeline is divided into 3 tasks. The first task is in data argumentation where it is decided what type of argumentation is used. This can be blur, rotation, noise, or a combination of these. The second task is then to use the argumentation on the MNIST data-set. Where we get the argumented MNIST data-set, which will be the input to the machine learning model. A side note is the size of the argumented data-set can be bigger than the original MNIST data-set.

\begin{figure}[htb!]
    \centering
    \input{figures/data-separation.tex}
    \caption{Data-set separation}
    \label{fig:data-set-sepa}
\end{figure}

In Figure~\ref{fig:data-set-sepa} describes how the data-set from MNIST will be devided. It will be devided into two data-sets, one for training and one for testing. The training data-set will be used to train the machine learning model. The default size is 80\% for training and 20\% for testing.