\section{Impact of memory limitations}\label{sec:impact_of_memory_limitations}
During development and testing, it was noticed that the memory of the available machines was a limiting factor. This section will discuss the impact of the memory limitations on the project, and how it affected the results and time to fit the models. It will be discussed whether or not the impact of limited memory was significant or if it was minor and could be ignored. If the impact was significant, what could be done to improve the situation.


\subsection{Impact on the results}\label{subsec:impact_on_results}
As stated throughout the project, a limiting factor when running the nonlinear models was the memory of available machines. This meant that it was impossible to properly run the model with the entire dataset when reducing the data with a nonlinear method; instead, only a smaller section of the dataset was used. The linear methods were not affected by this limitation, as they could be run on the entire dataset.

Using a smaller dataset could have an impact on the results of the final model when using nonlinear methods, but through the experiments done, it was noticed that the general trend of the models did not seem to change much after a certain number of samples.

Experiment four covered the effect different sample sizes had on the model's accuracy. \autoref{fig:experiment_4_performance_size} shows how at low sample sizes, the model with best accuracy can vary, but as the sample size increases, they all seem to fall into order. This would indicate that the different dimensionality methods are not significantly affected by the limited sample size, after a certain amount, and that the results should still be valid.

If however it was found that the results were significantly affected by the limited sample size, it might be considered finding a smaller dataset, use different methods to reduce the dimensionality, or try to gain access to a more powerfull machine that could handle the larger datasets, with nonlinear methods.


\subsection{Impact on time to fit the models}\label{subsec:impact_on_time_to_fit_the_models}
The time to fit the models was also affected by memory limitations. The models could use multiple CPU cores, but this also increased memory usage since each job required its own segment of memory to work with. Since nonlinear methods already used much memory, running the models on multiple cores was only possible on machines with enough memory.

An example of how much faster it was with multiple cores was when running \gls{isomap} in the second experiment. The model was attempted to be run on a machine with only 8 GB of memory, which was not enough to run on multiple cores. The model was then run on a machine with 32 GB of memory, which was enough to run the model on three cores. On the first attempt it took about 3+ days to run, whereas on the second attempt it took about one day to run. This shows the impact of using multiple cores, and how it can reduce the time to fit the models.

Had time been a significant factor for the project other than comparing the different methods, it would have been considered to run the models on a machine with more memory, as a powerful machine would have reduced the time to fit the models because it would allow the models to run with multiple cores. As it was, all models were run on a machine with 32 GB of memory, which was enough to run the models in acceptable time.

The impact of limited memory was significant, but it was not a major factor in the project. As the results were not significantly affected by the limited sample size. Instead, it was generally the time it took to fit the model that could be reduced by having access to more memory. However while while the time to fit the models was not of importance for this particular project, it did showcase the importance of choosing the right model for the machine.
