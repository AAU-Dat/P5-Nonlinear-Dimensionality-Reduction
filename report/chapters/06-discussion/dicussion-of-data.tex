\section{discussion of choice of data}
\todo{maybe this could be rewritten with the hypothesis that "}
Based on the results presented in Table \ref{tab:discussion-experiment-1-accuracy}, it appears that the \gls{mnist} dataset is somewhat well-suited for testing both linear and nonlinear dimensionality reduction methods. Thats due to it giving results which are inline with what we expected\todo{can i say this?}. Among the dimensionality reduction methods tested, \gls{kpca} and \gls{isomap} are nonlinear methods, and \gls{pca} and \gls{lda} are linear methods. \gls{kpca} achieved the highest accuracy with 15000 samples, but it is slower to train compared to the other methods. \gls{lda} had the fastest training time, but it had the lowest accuracy among the dimensionality reduction methods.

In general, the nonlinear dimensionality reduction methods performed similarly to the linear methods in terms of accuracy. \gls{kpca} had the highest accuracy, but it was slower to train compared to the linear methods. \gls{isomap} had a lower accuracy than the linear methods, but it was also slower to train. Overall, the results support the hypothesis that nonlinear dimensionality reduction methods work as well as linear methods on the \gls{mnist} dataset.

It is on the other hand possible that using a different dataset could have resulted in different conclusions regarding the performance of linear and nonlinear dimensionality reduction methods. The characteristics of the dataset, such as the number of data samples and the complexity of the data, can affect the performance of the different methods. For example, if the dataset has a larger number of data samples or is more complex, the nonlinear methods may outperform the linear methods in terms of accuracy. On the other hand, if the dataset is small or simple, the linear methods may perform better. Therefore, using a different dataset may have altered our conclusions about the performance of linear and nonlinear methods.

If the CIFAR-10 or Fashion-MNIST datasets had been used instead of the \gls{mnist} dataset, the results and conclusions regarding the performance of linear and nonlinear dimensionality reduction methods may have been different. Both the CIFAR-10 and Fashion-MNIST datasets are more complex than the \gls{mnist} dataset, with more data samples and more classes to classify. In general, nonlinear methods tend to perform better on complex and high-dimensional datasets compared to linear methods. Therefore, it is likely that the nonlinear dimensionality reduction methods would have outperformed the linear methods in terms of accuracy on the CIFAR-10 or Fashion-MNIST datasets. This could have led to different conclusions about the performance of linear and nonlinear methods.

It was also discussed the potential impact of using a different dataset on the performance of linear and nonlinear dimensionality reduction methods. The characteristics of the dataset, such as the number of data samples and the complexity of the data, can affect the performance of the different methods. Using a more complex and high-dimensional dataset may have led to different conclusions about the performance of linear and nonlinear methods. In conclusion, the \gls{mnist} dataset is well-suited for testing dimensionality reduction methods, and the results support the hypothesis that nonlinear dimensionality reduction methods work as well as linear methods on the \gls{mnist} dataset.