\chapter{Introduction}\label{cha:introduction}
% \todo[inline]{Chapter should probably contain: The initial problem (If we have one), motivation, and the scope or background of the project or theme. Report outline at the end.}
% \noindent
% In this project we will study some common methods of dimensionality reduction using the MNIST dataset for digit recognition.

% \paragraph{Keywords:} dimensionality reduction, linear methods, nonlinear methods, MNIST, Computer Vision (CV), Machine Learning, machine intelligence (artificial intelligence).

% Use sources \cite{IBM-machine-intelligence} and \cite{IBM-computer-vision} for superficial overview and explanations of umbrella terms.


Dimensionality reduction is an important topic in the field of \gls{ml}. The goal of dimensionality reduction is to reduce the number of features in a dataset while retaining as much relevant information as possible. This can be useful for improving the performance of \gls{ml} algorithms, as well as for visualizing and understanding the structure of a dataset~\cite{dimensionality-reduction-cheng}. In this report, we focus on the comparison of two different dimensionality reduction techniques: linear and nonlinear methods.

Linear methods for dimensionality reduction, such as \gls{pca}, are widely used and have been well studied. These methods are computationally efficient and often perform well on a variety of tasks~\cite{james-statistical-learning1}. However, nonlinear methods, such as \gls{kpca} are also relevant due to their ability to capture more complex structures in a dataset~\cite{dimensionality-reduction-cheng}. In this report, we aim to compare the relative effectiveness of these two types of dimensionality reduction methods.

We evaluate the performance of linear and nonlinear dimensionality reduction techniques on the \gls{mnist} dataset, a popular benchmark for image classification and recognition tasks. We use a \gls{svm} model to classify the images in the reduced datasets and compare the performance of the model under different settings. Our results provide insights into the relative effectiveness of linear and nonlinear methods for dimensionality reduction in the context of \gls{cv}.

Overall, this report is of interest to researchers and practitioners in the field of \gls{ml} and \gls{cv}. Our findings may have implications for the use of dimensionality reduction techniques in image classification and recognition tasks, and can be used for future research in this area. By comparing linear and nonlinear methods, we aim to shed light on the relative strengths and limitations of these techniques and provide guidance for their practical use.





% Notes to self: Humans vs computers in \gls{nn}. Why are humans good with little training, and computers only accceptable with much more training? Consider perhaps domains (recongnizing epsilon vs. recognizing a 3)

% Might be relevant to us: MNIST and fashion-MNIST (https://github.com/zalandoresearch/fashion-mnist).

\subsection*{On theory driven projects}
\blockcquote{Projectmodule}{The overall purpose of the project module is for the student to acquire the ability to analyze and evaluate the application of methods and techniques within database systems and / or machine intelligence to solve a specific problem. \textbf{This includes analyzes of the formal properties of the techniques and an assessment of these properties in relation to any requirements for the solution to the specific problem}. [...]

In this project module, the project work is primarily driven by theoretical and analytical considerations about the methods and techniques used. For a specific problem area, a project could, for example, be based on specific performance requirements for the developed software solution, and the project work can thus be guided by the solution's algorithmic time / space complexity as well as formal analyzes and considerations of its theoretical properties and performance guarantees.}

\input{chapters/01-introduction/motivation.tex}
\input{chapters/01-introduction/report-outline.tex}
