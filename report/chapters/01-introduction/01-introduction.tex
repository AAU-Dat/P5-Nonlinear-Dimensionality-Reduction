\chapter{Introduction}\label{cha:introduction}
Dimensionality reduction is an important topic in the field of \gls{ml}. The goal of dimensionality reduction is to reduce the number of features in a dataset while retaining as much relevant information as possible. This can be useful for improving the performance of \gls{ml} algorithms, as well as for visualizing and understanding the structure of a dataset~\cite{dimensionality-reduction-cheng}. In this report, we focus on the comparison of two different dimensionality reduction techniques: linear and nonlinear methods.

Linear methods for dimensionality reduction, such as \gls{pca}, are widely used and have been well studied. These methods are computationally efficient and often perform well on a variety of tasks~\cite{james-statistical-learning1}. However, nonlinear methods, such as \gls{kpca} are also relevant due to their ability to capture more complex structures in a dataset~\cite{dimensionality-reduction-cheng}. In this report, we aim to compare the relative effectiveness of these two types of dimensionality reduction methods.

We evaluate the performance of linear and nonlinear dimensionality reduction techniques on the \gls{mnist} dataset, a popular benchmark for image classification and recognition tasks~\cite{lecun-mnist-database}. We use a \gls{svm} model to classify the images in the reduced datasets and compare the performance of the model under different settings. Our results provide insights into the relative effectiveness of linear and nonlinear methods for dimensionality reduction in the context of \gls{cv}.

Overall, this report is of interest to researchers and practitioners in the field of \gls{ml} and \gls{cv}. Our findings may have implications for the use of dimensionality reduction techniques in image classification and recognition tasks, and can be used for future research in this area. By comparing linear and nonlinear methods, we aim to shed light on the relative strengths and limitations of these techniques and provide guidance for their practical use.


\input{chapters/01-introduction/motivation.tex}
