\section{Choice of dimensionatliy reduction}\label{sec:choice-of-dimensionality-reduction}

%Four dimensionality reduction methods
Some dimensionality reduction methods were presented in Section \ref{sec:dimensionality-reduction}. Due to the limited scope of the project not all of the dimensionality reduction methods will be chosen in the implementation. The methods are the following: \gls{pca}, \gls{lda}, \gls{isomap}, \gls{kpca}. The implementation of the methods will be based on scikit-learn's implementation of the methods.


\subsection{Linear methods}
The reason why we chose PCA was because the method has been widely used in research. It was used in the presentations.


In Tennenbaum PCA was referenced as the best. The point is that PCA had some good benchmarks and that was the choice of a method that could give good results by reducing the dimensions.

%PCA brugt, 
%relativt simpelt

The linear method \gls{fa} is a method that finds the latent variables in the data.


\gls{nmf} is a method that also finds latent

\subsection{Nonlinear methods}
The reason why T-SNE is not chosen is because it can not be used for a model, which disregards the possiblity of using the method in order to possibly improve the machine learning model.


\gls{kpca} was chosen because it is the interesting property of being able to 






%Hyperparameters