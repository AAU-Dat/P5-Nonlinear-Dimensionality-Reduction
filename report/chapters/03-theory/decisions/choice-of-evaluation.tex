\subsection{Choices of evaluation metrics}\label{subsec:choice-of-evaluation}
As mentioned in \autoref{subsec:metrics}, many different metrics can be used to evaluate a model. The basic metrics used in this project were accuracy, precision, recall, and F1. These metrics each have their strengths and weaknesses. Some of these strengths and weaknesses of the metrics were described in \autoref{subsec:metrics}.

Another metric used to measure the performance of the models was the time it took to train them. Depending on the circumstances, the time it takes to train a model could be just as important as the accuracy of the model, depending on the loss of accuracy. Such as, if a model takes 10 minutes to train but only loses 1\% accuracy, it might be worth using that model compared to another model that takes several hours or days.

For the pipeline of this project, the metric used to evaluate the models was f1-score. The reason is that accuracy would not be a great indicator on its own, since the training data contains more examples of some digits than others. F1-score is a good metric to use in this case, since it is a weighted average of precision and recall. This means that it is not as affected by the imbalance of the training data as the other metrics.