\subsection{Choice of Preprocessing}
Following the theory of preprocessing from \ref{sec:preprocessing}, this section will cover the decisions made for preprocessing the data and how there were taken.

As stated in section \ref{subsec:preprocessing-steps}, the amount of preprocessing needed varies depending on what is needed for the model. For this project, it was deemed sufficient to reshape the data, normalize it, and augment it. A detailed description of each decision can be found in the following sections.

\subsubsection{Reshape}
This subsection describes the decisions made regarding reshaping the data loaded from the \gls{mnist} dataset. This subsection only describes the decisions for the training data as an example, but similar decisions were also made for the test data.

The data, when loaded in at first, is shaped as a long array of 47.040.000 integers, ranging from 0-255, representing the values of the pixels in all 60.000 images, and then another array of 60.000 integers representing what number a given image is, between 0-9. The images are in a 28x28 matrix, meaning that the first 784 integers represent the first image, and the successive 784 integers represent the second image.

Additionally, the function used from \gls{sklearn} did not accept the data in this format because the function \texttt{fit} expects: \textcquote{scikit-learn-PCA}{X: array-like of shape (n\_samples, n\_features)}, which means that the function expects the data to be in array of samples(images), and an array of features(pixels). Because of the input \texttt{fit} expects, it was necessary to reshape the data into a 60.000x784 matrix. From this, there was a clear distinction between each image, and the data could easily be used in the functions given by \gls{sklearn}.

\subsubsection{Normalization}
In \autoref{sec:normalization}, two methods of normalizing data are described: min-max scaler and variance scaler. From these two methods, both are chosen. The combination of normalization is called StandardScaler, and is a commonly used method of normalizing data, it was deemed a good choice. Variance scaler also ensures that it is comparable to prevent bias in the model~\cite{StandardScaler-towardsAi}. Both methods would have been good choices for normalization, variance scaler was chosen, but either would be acceptable for this project.

\subsubsection{Data augmentation}
As mentioned in \autoref{subsec:data-augmentation}, data augmentation increases the amount of data and makes the model more general. Augmentation creates new data from the existing data by applying different augmentations to the original data and using the new data to train the model. For this project, rotation augmentations were chosen. They were also deemed to represent real-world errors in data. For example a number could become more or less rotated due to a writers writing style. These are some of the cases thought of when deciding on the augmentations.
