\section{Evaluation and metrics}\label{sec:evalueation}

A pipeline is generally evaluated based on quantitative metrics in machine learning and data science. These metrics are in classification general based on the confusion matrix. An example of a confusion matrix can be seen in \todo{need to make pictue}. It represents all the guesses a ml model has made, where the x-axis, in this case, is the true class of a given data point, and the y-axis is the class the model has guessed the data point to have. The numbers are how many times the model has guessed the different combinations. \cite{james-statistical-learning}

\subsection*{Metrics}

There exist many metrics, but generally, there are four basic metrics. These are accuracy, precision, recall, and f1. Outside these, there are more esoteric metrics like the Mattheus correlation coefficient, Cohen's kappa, and more. The following section will describe each of the four basic metrics. \cite{metrics-for-multi}

Accuracy is the most straightforward of the metrics. It describes the percentage of correct guesses out of all guesses. This metric works well in cases where the sizes of the different classes are similar. It struggles in cases where one class is much larger than another. Here It is possible to achieve very high accuracy by only guessing the larger class, which can make a model seem impressive while simultaneously not doing what it is supposed to if it is crucial to find the smaller class.

In the case where one class is smaller is where the precision metric comes in handy. It is calculated by taking the correct guesses for a class and dividing them by all guesses on this class, which includes wrong guesses. This gets around the problems accuracy has with classes with very different sizes because it is not possible to always guess the bigger class and get a good precision.

The third metric is recall, primarily used in cases where it really matters if one class is guessed correctly. Recall could be in cases like corona, where a false positive is much better than a false negative. Recall is calculated by dividing the true positives by true positives and false negatives.

The last basic metric is f1. Its commonly used when both recall and precision are essential. It is calculated two times precision times recall decided by precision plus recall.


In a pipeline, these metrics are generally used for hyperparameter tuning and general evaluation of the model's performance. Here hyperparameter tuning will generally only use a single metric to focus on to maximize it. In evaluation, on the other hand, the metrics are used more generally to explain what the model does well and what it does not do well. \cite{james-statistical-learning}