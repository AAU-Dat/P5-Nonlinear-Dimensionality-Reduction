\section{Evaluation and metrics}\label{sec:evalueation}

In machine learning and data science a pipeline is generally evaluated based on quantitative metrics. These metrics are in classification general based on the confusion matrix. An example of a confusion matrix can be seen in \todo{need to make pictue}. It represents all the guesses a ml model has made, where the x axis in this case is the true class of a given data point and the y axis is the class the model has guessed the datapoint to have. The numbers are then how many times the model has guessed the different combinations. \cite{james-statistical-learning}

\subsection*{Metrics}

There exist many metrics, but generally there are 4 basic metrics. These are accuracy, precision, recall, f1. Outside these there are more esoteric metrics like Mattheus correlation coefficient and Cohen's kappa and more. In the following section each of the 4 basic metrics will be described. \cite{metrics-for-multi}

Accuracy is the most straight forward of the metrics. It describes the percentage of correct guesses out of all guesses. This metric works well in cases where the sizes of the different class are similar. It struggles in case where one class is much larger than another. Here It's possible to achieve very high accuracy by only guessing the larger class which can make a model seem impressive while simultaneously not really doing what you want if It's important to find the smaller class.

This is where the precision metric comes in handy. It is calculated by taking the correct guesses for a class and dividing by all guesses on this class, this includes guesses that where wrong. This gets around the problems' accuracy has with classes that have very different size because it's not possible to always just guess the bigger class and get a good precision.

The third metric is recall, its most used in cases where it really matters if one class is guessed correct. This could be in cases like corona where a false positive is much better than a false negative. Recall is calculated by taking the true positives and dividing by true positives plus false negatives.

The last basic metric is f1. Its commonly used when both recall and precision are important. Its calculated 2 times precision times recall decided by precision plus recall.


In a pipeline these metrics are generally both used for hyperparameter tuning and general evaluation of the performance of the model. Here hyperparameter tuning will generally only use a single simple metric to focus on to maximize it. In evaluation on the other hand the metrics are used more generally to explain what the model does well and what it doesn't do well. \cite{james-statistical-learning}