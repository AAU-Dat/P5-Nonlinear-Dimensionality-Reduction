\section{Dimensionality reduction}\label{sec:dimensionality-reduction}
In general dimensionality reduction is the transformation of data with many features (dimensions) into a new representation of the data with fewer features, while preserving as much relevant information as possible. This is done by finding a transformation of the data that maps the data to some lower dimensional space while preserving the meaningful distances between the data points~\cite{dimensionality-reduction-comparative-review}.

Dimensionality reduction can be divided into two categories: linear and non-linear methods. Linear methods are based on linear algebra, and non-linear methods are not. This section presents some of the common methods from both categories.


\subsection{Linear methods}\label{subsec:linear-methods}
The linear methods covered are: \gls{pca}, \gls{fa}, \gls{nmf}, and \gls{lda}.


\subsubsection{Principal Components Analysis}\label{subsubsec:principal-components-analysis}
\gls{pca} is a linear method that is used to reduce the dimensionality of a dataset by projecting it onto a lower dimensional subspace, retaining as much of the variance as possible. The best projection is found through the directions of maximum variance in the data based on the covariance matrix, and then projecting the data onto those directions. The directions of maximum variance are called principal components, and the projection is the result of the \gls{pca}~\cite{dimensionality-reduction-comparative-review}.


\subsubsection{Factor Analysis}\label{subsubsec:factor-analysis}
\gls{fa} is very similar to \gls{pca}, and \gls{pca} may be called a type of \gls{fa}. However, \gls{fa} is based on the assumption that the data is generated by a linear combination of a few latent variables called factors, and that the observed variables are a linear combination of the latent variables plus some noise.

The goal of \gls{fa} is to find the factors that explain the most covariance in the data. Intuitively, related items have stronger mathematical correlations, and can thus be grouped with less loss of information. Once the correlations are determined, a rotation is performed to make the factors easier to interpret~\gls{fa}~\cite{decoster-1998-factor-analysis-overview}.


\subsubsection{Non-negative matrix factorization}\label{subsubsec:non-negative-matrix-factorization}
\gls{nmf} constructs approximate factorizations of a non-negative data matrix $V$ into two non-negative matrices $W$ and $H$ such that $V \approx WH$. The non-negativity constraints permit the intuition that data may be represented as parts forming a whole, as a result can be thought of as a sum of parts~\cite{lee-1999-learning-nmf}. For example, a text document may be represented as a combination of topics, and each topic may be represented as a combination of words.


\subsubsection{Linear Discriminant Analysis}\label{subsubsec:linear-discriminant-analysis}

\gls{lda} is quite similar to \gls{pca} since they both try to project data into a hyperplane but instead of picking the hyper plane to keep the most variance \gls{lda} tries to maximize something else. \gls{lda} finds the hyper plane which has the most separation between the classes and also keeping the data points with the same class as close together as possible. This is a three step process. First separability between different classes (between-class variance) is calculated as the distance between the means of each class. Secondly the distance between the mean and samples of each class (within-class variance) is calculated. Third step is creating a lower dimensional representation that maximizes the between-class variance and minimizes the within-class variance~\cite{linear-discriminant-analysis-tutorial}.


\subsection{Non-linear methods}\label{subsec:non-linear-methods}
Sometimes high-dimensional data may contain non-linear relationships, which linear methods cannot capture. It has been shown that \gls{pca} fails to find meaningful components in the swiss-roll dataset~\cite{tennenbaum}. In such cases non-linear methods are used. The non-linear methods covered are: \gls{kpca}, \gls{mds}, and \gls{isomap}.




\todo[inline]{maybe whe should have a introduction to manifold learning as a precurser}


\subsubsection{Kernel Principal Component Analysis}\label{subsubsec:kernel-principal-component-analysis}
\gls{kpca} is an extension of \gls{pca}, and it projects the data onto a higher dimensional plane, where \gls{pca} is performed. \gls{kpca} uses a kernel function $\phi (x)$ which is used to obtain to what would correspond as the covariance matrix in \gls{pca}in the kernel space $K$, from which eigenvalue decomposition can be computed, without computing the actual kernel function $\phi (x)$, which otherwise would be computationally expensive. Thus, \gls{kpca} applies a kernel function (some examples being Guassian or polynomial kernels) to the data set $ \{x_{i} \}$, constructs a kernel matrix. From the kernel matrix one can compute the Gram matrix, from which, similar to \gls{pca}, one can make use of eigen-decomposition to get the kernel $k$ components~\cite{kernel-pca}.


\subsubsection{Isometric Feature Mapping}\label{subsubsec:isometric-feature-mapping}
\gls{isomap} is another non-linear method that is a special case of \gls{cmds}. The former method preserves the geodesic distance, while the latter performs dimensionality reduction, which is why \gls{isomap} can be considered an extension of \gls{mds}. The geodesic distance is calculated by first constructing a neighborhood graph for every data point connected to its $k$ point, which is done with K-nearest neighbors algorithm. After constructing the neighborhood graph, the distance then can be calculated with a shortest path algorithm to determine the distances on the graph which correspond to the distances on the manifold. This results in a distance matrix, which is used as the input for \gls{cmds} \cite{Multidimensional-Scaling-Sammon-Mapping-and-Isomap}.

\gls{cmds} is a branch of \gls{mds} which is used as a part of isomap. \gls{cmds} tries to preserve the similarity between the data points from the higher dimensional space that are to be embedded onto the lower dimensional space. This is achieved by performing eigen-decomposition on the matrix input. \gls{cmds} resembles \gls{pca} -- one can think of \gls{mds} as being a family of methods while \gls{pca} is just a method~\cite{difference-between-pca-and-mds} \cite{Multidimensional-Scaling-Sammon-Mapping-and-Isomap}.

It is assumed that \gls{isomap} is suited to discover manifolds of arbirary dimensionality, and that it guarantees an approximation of the true structure of the manifold~\cite{tennenbaum}.


\subsubsection{Multi Dimensional Scaling}\label{subsubsec:multi-dimensional-scaling}
Multi Dimensional scaling is a non-linear method that is used to reduce the dimensionality of a dataset by projecting it onto a lower dimensional subspace, while preserving as much of the variance between the data as possible. The projection is found by calculating the best from the Multi Dimensional Scaling function which is a modified version of the STRESS function. The STRESS function finds the least change in data when in the case of \gls{mds} is when reducing the dimensionality~\cite{multi-dimensional-scaling-leeuw}.



4 different dimensionality reduction methods have been chosen to be in the implementation. This is 2 linear and 2 nonlinear methods, in particular \gls{pca}, \gls{lda}, \gls{kpca} and \gls{isomap}. These 4 where chosen because they cover the spectrum of options well and are some of the most used. Only 4 where chosen to keep the focus of the project tight and the right amount of depth versus breath of the project.




% In this project we will focus on dimensionality reduction for the purpose of computer vision, and in particular for the MNIST dataset.


% In this project we will study some common methods of dimensionality reduction using the MNIST dataset for digit recognition.