\section{Dimensionality reduction}\label{sec:dimensionality-reduction}
In general, dimensionality reduction transforms data with many features (dimensions) into a new representation of the data with fewer features while preserving as much relevant information as possible. Dimensionality reduction is made by finding a transformation of the data that maps the data to some lower dimensional space while keeping the mean distances between the data points~\cite{dimensionality-reduction-comparative-review}.

Dimensionality reduction can be divided into two categories: linear and nonlinear methods, described in Section \ref{sec:linear-vs-nonlinear}. This section presents some of the standard methods from both categories.

\todo[inline]{Linear methods are based on the assumption that the data is linear. 
A linear dataset is one in which all possible prediction outcomes can be obtained by combinations of the original features without any additional interactions. 
You can show an example of linear data for regression and classification. 
Note that this difference is not so strict as some methods could be thought of as both}


\subsection{Linear methods}\label{subsec:linear-methods}
The linear methods covered are: \gls{pca}, \gls{fa}, \gls{nmf}, and \gls{lda}.


\subsubsection{Principal Components Analysis}\label{subsubsec:principal-components-analysis}
\gls{pca} is a linear method used to reduce a dataset's dimensionality by projecting it onto a lower dimensional subspace, retaining as much of the variance as possible. The best projection is found through the directions of maximum variance in the data based on the covariance matrix and then projecting the data onto those directions. The directions of maximum variance are principal components, and the projection results from the \gls{pca}~\cite{dimensionality-reduction-comparative-review}.


\subsubsection{Factor Analysis}\label{subsubsec:factor-analysis}
\gls{fa} is very similar to \gls{pca}, and \gls{pca} may be called a type of \gls{fa}. However, \gls{fa} is based on the assumption that the data is generated by a linear combination of a few latent variables called factors and that the observed variables are a linear combination of the latent variables plus some noise.

The goal of \gls{fa} is to find the factors that explain the most covariance in the data. Intuitively, related items have stronger mathematical correlations and can thus be grouped with less loss of information. Once the correlations are determined, a rotation is performed to make the factors easier to interpret~\gls{fa}~\cite{decoster-1998-factor-analysis-overview}.


\subsubsection{Non-negative matrix factorization}\label{subsubsec:non-negative-matrix-factorization}
\gls{nmf} constructs approximate factorizations of a non-negative data matrix $V$ into two non-negative matrices $W$ and $H$ such that $V \approx WH$. The non-negativity constraints permit the intuition that data may represent parts forming a whole, which can be thought of as a sum of parts~\cite{lee-1999-learning-nmf}. For example, a text document may represent a combination of topics, and each may represent a combination of words.


\subsubsection{Linear Discriminant Analysis}\label{subsubsec:linear-discriminant-analysis}

\gls{lda} is quite similar to \gls{pca} since they both try to project data into a hyperplane. Still, instead, \gls{lda} tries to maximize class separability, making it easier to contrast classes. \gls{lda} finds the hyperplane with the most separation between the classes and keeps the data points with the same class as close together as possible. This is a three-step process. The first step is to determine the separation between different classes (between-class variance) as the distance between the means of each class. Secondly, the distance between the mean and samples of each class (within-class variance) is calculated. The third step is creating a lower dimensional representation that maximizes the between-class variance and minimizes the within-class variance~\cite{linear-discriminant-analysis-tutorial}.


\subsection{Non-linear methods}\label{subsec:non-linear-methods}
Sometimes high-dimensional data may contain non-linear relationships, which linear methods cannot capture. It has been shown that \gls{pca} fails to find significant components in the swiss-roll dataset~\cite{tennenbaum}. In such cases, non-linear methods are used. The non-linear methods covered are \gls{kpca}, \gls{mds}, and \gls{isomap}.


\subsubsection{Kernel Principal Component Analysis}\label{subsubsec:kernel-principal-component-analysis}
\gls{kpca} is an extension of \gls{pca}, and it projects the data onto a higher dimensional plane, where \gls{pca} is performed. 
The first step is to construct the kernel matrix from the data. The kernel matrix is a matrix of the dot products of the data points, the kernel matrix is used like the covariance matrix in \gls{pca}~\cite{kernel-pca}. In the kernel matrix the data is in a higher dimensional space, where the data is linear separable. By using the kernel function, \gls{kpca} exposes the kernel trick, which is instead of calculating the dat into a higher dimensional space, it calculates the inner product between the datapoints, which is computationally cheap to calculate instead of calculating every datapoint into a higher dimensional space.
The kernel matrix is then centered by subtracting the mean of each row and column. This matrix is also called the Gram matrix. The Gram matrix is then used to find eigenvectors and eigenvalues. By using the kernel trick, \gls{kpca} can compute eigenvalue decomposition, as in \gls{pca}.The eigenvectors are then used to project the data onto a higher dimensional space, where \gls{pca} is performed. The eigenvectors with the highest eigenvalues are the principal components, and the projection is the data projected onto the principal components~\cite{kernel-pca}. 

\subsubsection{Isometric Feature Mapping}\label{subsubsec:isometric-feature-mapping}
\gls{isomap} is another non-linear method that is a special case of \gls{cmds}. It is assumed that \gls{isomap} is suited to discover manifolds of arbitrary dimensionality and that it guarantees an approximation of the true structure of the manifold~\cite{tennenbaum}.

The first step in \gls{isomap} is to map the data points in a graph. The graph is constructed by connecting each point to its nearest neighbors. The user sets the number of nearest neighbors. The nearest neighbors are found by using the Euclidean distance. The Euclidean distance is the linear distance between two points in a Euclidean space~\cite{Multidimensional-Scaling-Sammon-Mapping-and-Isomap}.

In the second step, \gls{isomap} finds the Geodesic distance between each point~\cite{Multidimensional-Scaling-Sammon-Mapping-and-Isomap}. The Geodesic distance is the shortest distance between two points on the surface of a sphere. The Geodesic distance is calculated by finding the shortest path between two points on a graph. The graph is constructed by connecting each point to its nearest neighbors. The shortest path can be found by using the Dijkstra algorithm~\cite{multi-dimensional-scaling-leeuw}.

The final step in \gls{isomap} is finding the data's \gls{mds} projection. The \gls{mds} projection is found by minimizing the stress function. The stress function is the sum of the squared differences between the distances in the original data and the distances in the projected data~\cite{multi-dimensional-scaling-leeuw}, so the stress function finds the slightest change in the data. When in the case of \gls{isomap} is when reducing the dimensions. 



\subsection{Choice of dimensionality reduction}
Four different dimensionality reduction methods have been chosen to be in the implementation. These are linear and nonlinear methods, particularly \gls{pca}, \gls{lda}, \gls{kpca}, and \gls{isomap}. The group chose these four because they cover the spectrum of options well and are some of the most used. Only four were selected to keep the project's focus tight and the right amount of depth versus breadth of the project.

%ISOMAP 


%The geodesic distance is used in \gls{isomap} to find the shortest distance between two points in a dataset. %These will be clarified first before clarifying \gls{isomap}. 

%\subparagraph{Multi Dimensional Scaling}\label{subsec:multi-dimensional-scaling}
%\gls{mds} is a non-linear method that is used to reduce the dimensionality of a dataset by projecting it onto a lower dimensional subspace, while preserving as much of the variance between the data as possible. The projection is found by calculating the best result from the \gls{mds} function which is a modified version of the STRESS function. The STRESS function finds the least change in data when in the case of \gls{mds} is when reducing the dimensionality~\cite{multi-dimensional-scaling-leeuw}.

%\subparagraph{Geodesic distance}\label{subsec:geodesic-distance}
%Geodesic is a distance between two points on a surface of a sphere. The geodesic distance between two points is the shortest distance between the two points on the surface of a sphere. The geodesic distance is calculated by finding the shortest path between two points on a graph. The graph is constructed by connecting each point to its nearest neighbors. The shortest path is found by using the Dijkstra's algorithm~\cite{multi-dimensional-scaling-leeuw}.


% In this project we will focus on dimensionality reduction for the purpose of computer vision, and in particular for the MNIST dataset.


% In this project we will study some common methods of dimensionality reduction using the MNIST dataset for digit recognition.