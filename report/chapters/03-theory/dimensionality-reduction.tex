\section{Dimensionality reduction}\label{sec:dimensionality-reduction}
In general, dimensionality reduction transforms data with many features (dimensions) into a new representation of the data with fewer features while preserving as much relevant information as possible. Dimensionality reduction is made by finding a transformation of the data that maps the data to some lower dimensional space while keeping the mean distances between the data points~\cite{dimensionality-reduction-comparative-review}.

Dimensionality reduction can be divided into two categories: linear and nonlinear methods, described in Section \ref{sec:linear-vs-nonlinear}. This section presents some of the standard methods from both categories.

\todo[inline]{Linear methods are based on the assumption that the data is linear. 
A linear dataset is one in which all possible prediction outcomes can be obtained by combinations of the original features without any additional interactions. 
You can show an example of linear data for regression and classification. 
Note that this difference is not so strict as some methods could be thought of as both}


\subsection{Linear methods}\label{subsec:linear-methods}
The linear methods covered are: \gls{pca}, \gls{fa}, \gls{nmf}, and \gls{lda}.


\subsubsection{Principal Components Analysis}\label{subsubsec:principal-components-analysis}
\gls{pca} is a linear method used to reduce a dataset's dimensionality by projecting it onto a lower dimensional subspace, retaining as much of the variance as possible. The best projection is found through the directions of maximum variance in the data based on the covariance matrix and then projecting the data onto those directions. The directions of maximum variance are principal components, and the projection results from the \gls{pca}~\cite{dimensionality-reduction-comparative-review}.


\subsubsection{Factor Analysis}\label{subsubsec:factor-analysis}
\gls{fa} is very similar to \gls{pca}, and \gls{pca} may be called a type of \gls{fa}. However, \gls{fa} is based on the assumption that the data is generated by a linear combination of a few latent variables called factors and that the observed variables are a linear combination of the latent variables plus some noise.

The goal of \gls{fa} is to find the factors that explain the most covariance in the data. Intuitively, related items have stronger mathematical correlations and can thus be grouped with less loss of information. Once the correlations are determined, a rotation is performed to make the factors easier to interpret~\gls{fa}~\cite{decoster-1998-factor-analysis-overview}.


\subsubsection{Non-negative matrix factorization}\label{subsubsec:non-negative-matrix-factorization}
\gls{nmf} constructs approximate factorizations of a non-negative data matrix $V$ into two non-negative matrices $W$ and $H$ such that $V \approx WH$. The non-negativity constraints permit the intuition that data may represent parts forming a whole, which can be thought of as a sum of parts~\cite{lee-1999-learning-nmf}. For example, a text document may represent a combination of topics, and each may represent a combination of words.


\subsubsection{Linear Discriminant Analysis}\label{subsubsec:linear-discriminant-analysis}

\gls{lda} is quite similar to \gls{pca} since they both try to project data into a hyperplane. Still, instead, \gls{lda} tries to maximize class separability, making it easier to contrast classes. \gls{lda} finds the hyperplane with the most separation between the classes and keeps the data points with the same class as close together as possible. This is a three-step process. The first step is to determine the separation between different classes (between-class variance) as the distance between the means of each class. Secondly, the distance between the mean and samples of each class (within-class variance) is calculated. The third step is creating a lower dimensional representation that maximizes the between-class variance and minimizes the within-class variance~\cite{linear-discriminant-analysis-tutorial}.


\subsection{Non-linear methods}\label{subsec:non-linear-methods}
Sometimes high-dimensional data may contain non-linear relationships, which linear methods cannot capture. It has been shown that \gls{pca} fails to find significant components in the swiss-roll dataset~\cite{tennenbaum}. In such cases, non-linear methods are used. The non-linear methods covered are \gls{kpca}, \gls{mds}, and \gls{isomap}.

\todo[inline]{maybe whe should have a introduction to manifold learning as a precurser}

\subsubsection{Kernel Principal Component Analysis}\label{subsubsec:kernel-principal-component-analysis}
\gls{kpca} is an extension of \gls{pca}, and it projects the data onto a higher dimensional plane, where \gls{pca} is performed. 
The first step is to construct the kernel matrix from the data. The kernel matrix is a matrix of the dot products of the data points, the kernel matrix is used like the covariance matrix in \gls{pca}~\cite{kernel-pca}. In the kernel matrix the data is in a higher dimensional space, where the data is linear separable. By using the kernel function, \gls{kpca} exposes the kernel trick, which is instead of calculating the dat into a higher dimensional space, it calculates the inner product between the datapoints, which is computationally cheap to calculate instead of calculating every datapoint into a higher dimensional space.
The kernel matrix is then centered by subtracting the mean of each row and column. This matrix is also called the Gram matrix. The Gram matrix is then used to find eigenvectors and eigenvalues. By using the kernel trick, \gls{kpca} can compute eigenvalue decomposition, as in \gls{pca}.The eigenvectors are then used to project the data onto a higher dimensional space, where \gls{pca} is performed. The eigenvectors with the highest eigenvalues are the principal components, and the projection is the data projected onto the principal components~\cite{kernel-pca}. 

\subsubsection{Isometric Feature Mapping}\label{subsubsec:isometric-feature-mapping}\todo[inline]{write about isomap}
\gls{isomap} is another non-linear method that is a special case of \gls{cmds}. The former method preserves the geodesic distance, while the latter performs dimensionality reduction, which is why \gls{isomap} can be considered an extension of \gls{mds}. The geodesic distance is calculated by first constructing a neighborhood graph for every data point connected to its $k$ point, which is done with K-nearest neighbors algorithm. After constructing the neighborhood graph, the distance then can be calculated with a shortest path algorithm to determine the distances on the graph which correspond to the distances on the manifold. This results in a distance matrix, which is used as the input for \gls{cmds} \cite{Multidimensional-Scaling-Sammon-Mapping-and-Isomap}.

\gls{cmds} is a branch of \gls{mds} which is used as a part of isomap. \gls{cmds} tries to preserve the similarity between the data points from the higher dimensional space that are to be embedded onto the lower dimensional space. This is achieved by performing eigen-decomposition on the matrix input. \gls{cmds} resembles \gls{pca} -- one can think of \gls{mds} as being a family of methods while \gls{pca} is just a method~\cite{difference-between-pca-and-mds} \cite{Multidimensional-Scaling-Sammon-Mapping-and-Isomap}.

It is assumed that \gls{isomap} is suited to discover manifolds of arbirary dimensionality, and that it guarantees an approximation of the true structure of the manifold~\cite{tennenbaum}.


% \subsubsection{Multi Dimensional Scaling}\label{subsubsec:multi-dimensional-scaling}
% Multi Dimensional scaling is a non-linear method that is used to reduce the dimensionality of a dataset by projecting it onto a lower dimensional subspace, while preserving as much of the variance between the data as possible. The projection is found by calculating the best from the Multi Dimensional Scaling function which is a modified version of the STRESS function. The STRESS function finds the least change in data when in the case of \gls{mds} is when reducing the dimensionality~\cite{multi-dimensional-scaling-leeuw}.


\subsection{Choice of dimensionality reduction}
Four different dimensionality reduction methods have been chosen to be in the implementation. These are two linear and two nonlinear methods, in particular \gls{pca}, \gls{lda}, \gls{kpca}, and \gls{isomap}. The group chose these four because they cover the spectrum of options well and are some of the most used. Only four were selected to keep the project's focus tight and the right amount of depth versus breadth of the project.




% In this project we will focus on dimensionality reduction for the purpose of computer vision, and in particular for the MNIST dataset.


% In this project we will study some common methods of dimensionality reduction using the MNIST dataset for digit recognition.