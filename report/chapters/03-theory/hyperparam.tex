\section{Hyperparameter optimization}\label{sec:hyperparam}
This section introduces the concept of hyperparameter optimization and the importance of this process in machine learning.


\subsection{Hyperparameters}\label{subsec:hyperparam-what}
Hyperparameters are the parameters set, for algorithms, before training the model, which does not get learned from the data. The values of hyperparameters can have a significant impact on the performance of the model. How hyperparameters differ from model parameters is that those model parameters get learned from the data during model training~\cite{probst2019tunability}.

Hyperparameters exist for both \gls{fe} and for \gls{ml} models, examples of these could be \gls{svm} and \gls{pca}. Here \gls{pca} should at least have the hyperparameter which corresponds to the amount of dimensions it should reduce down to. \gls{svm} would have hyperparameters like which kind of kernel it should use~\cite{probst2019tunability}.

\subsection{Methods of optimization for hyperparameters}\label{subsec:hyperparam-how}
Usually, when selecting hyperparameters for a given algorithm, users can resort to default values based on the algorithm's documentation, read literature for recommendations, or try different values and see which one works best. However, this approach could be more efficient, as it is time-consuming and requires a lot of manual work~\cite{probst2019tunability}.

Instead, the process of finding optimal hyperparameters can be given to the computer~\cite{automated-machine-learning}, given a set of configurations. Hyperparameter optimization is complex because it is unknown which hyperparameters will significantly affect the model, which hyperparameters will interact with each other, and how their interactions will change the model's performance. According to Marc Claesen~\cite{hyperparam-search}, the number of hyperparameters that have a significant impact may be small, but that does not mean that the number of meaningful combinations may be small too.

Many different techniques can get used to optimize the hyperparameters automatically~\cite{automated-machine-learning}. An example of such a technique is grid search. This technique allows the user to define \textcquote{automated-machine-learning}{a set of finite values for each hyperparameter, and grid search evaluates the Cartesian product of these sets}.

Another example of a method is random search, a technique similar to grid search. However, instead of evaluating all the combinations of hyperparameters, it randomly evaluates hyperparameters, given a limited amount of time. Both methods have limitations; grid search is inefficient when the number of hyperparameters is significant due to the curse of dimensionality, which means that for algorithms that require large amounts of hyperparameters, it is not feasible to use this technique~\cite{yang2020hyperparameter}.

Random search can prove helpful, but there is no guarantee that it will find the optimal hyperparameters, given its limited time to evaluate them. Knowing the optimal time limit for random search is tricky, as it depends on the number of hyperparameters, the number of possible values for each hyperparameter, and the number of times the hyperparameters are evaluated~\cite{yang2020hyperparameter}.

From this, it can be concluded that no single method is optimal for all cases. The optimal method depends on many variables, such as the number of hyperparameters and possible values for each hyperparameter. Therefore, it is essential to evaluate the different methods and find the one that works best for the given problem. Of course, many other techniques could be discussed. Nevertheless, these two methods should sufficiently demonstrate the strengths and weaknesses of different techniques.

\input{chapters/03-theory/decisions/choice-of-hyperparameter.tex}
