\section{Classification}\label{sec:classification}
%What is classification?
In \gls{ml} classification is used when a quantitative answer is not required. Instead, the goal is to classify the input into one of a set of categories, set in the data-set.As long as the data-set support supervised-learning, classification can be applied to it.  

This is done by training a model on a set of labeled data, and then using the model to predict the category of new, unlabeled data. 

This means that the training data consists of input and class label pairs. The input can be a single value, such as a number or a string, or a vector of values, such as a list of numbers or a list of strings. The input can also be a matrix of values, such as a grayscale image or a color image. The class label is a discrete value, such as a number or a string. 

The model is trained to minimize the error between the predicted class label and the actual class label. The error is calculated by comparing the predicted class label with the actual class label. 

The model is trained by finding the best parameters for the model, such as the weights of a neural network or parameters in models, such as Support Vector Machines. These parameters are then used to predict the category of new data. The process of finding the best parameters is called Hyperparameter optimization, this will be discussed further in Section~\ref{sec:hyperparam}.

In classification there is two types of classification, binary classification and multi-class classification. Binary classification is the classification of two classes, while multi-class classification is the classification of more than two classes. The \gls{mnist} dataset, which is used in this project, presents a multi-class classification problem, as the images can represent any of the 10 digits. The \gls{svm} model however is a binary classification model, and thus has to be adapted to the multi-class classification problem. There are two approaches to this problem: \gls{ovo} and \gls{ova}.


%hvordan classificerer man

%hvad er multi-class classification

%hvad er binary classification

%binary vs multi-class classification

%hvad er f√¶lles for classifications algoritmerne

%hvad er ovo
\gls{ovo} is a method where the model is trained on all possible combinations of two classes. For example, if there are 5 classes, the model is trained on 10 different models, one for each combination of two classes, this makes it computationally expensive as it has to go througth every combination. The model is then evaluated on all the models, and the class with the highest score is chosen as the predicted class~\cite{james-statistical-learning}.

%hvad er ova
\gls{ova} however is a method where the model is trained faster than in \gls{ovo}, as it only uses one class to distinguish if the data is similar or not. For example, if there are 5 classes, the model is trained on 5 different variations of the model, one for each class. This makes \gls{ova} good to distinguish between the current class that is being modeled from the other classes, however in \gls{ova} it is harder to distinguish between the other classes that is not being trained on. The model is then evaluated on all the models, and the class with the highest score is chosen as the predicted class~\cite{james-statistical-learning}.

%Our choice of ovo and ova
\gls{ovo} is more computationally expensive than \gls{ova}, but is more accurate. The choice of \gls{ovo} or \gls{ova} is therefore a trade-off between accuracy and computational cost~\cite{james-statistical-learning}. The \gls{svm} model is chosen because it is a relatively simple model, and thus \gls{ova} is chosen as it is faster than \gls{ovo}.


%not used yet

%in {james-statistical-learning} use this as link instead https://hastie.su.domains/ISLR2/ISLRv2_website.pdf