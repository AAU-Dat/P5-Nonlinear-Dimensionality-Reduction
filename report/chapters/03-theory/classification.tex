\section{Classification}\label{sec:classification}
When making a model, it is only sometimes possible to predict the value of a variable.  For example, if one wants to predict a house's price, one can not predict the exact price, but may be able to predict if the price is high or low. This is called classification. In \gls{ml}, classification is when a quantitative answer is not required; a qualitative answer is satisfying. The goal is to classify the input into one of a set of categories set in the dataset. As long as the dataset supports supervised learning, classification can be applied to it~\cite{james-statistical-learning1}.
This is done by training a model on a labeled dataset and then using the model to predict the category of new, unlabeled data~\cite{james-statistical-learning1}.

On the other hand when it is possible to predict the value of a variable its in machine learning called regression. Regression is a type of supervised learning where the goal is to predict a continuous numerical value, such as the price of a house. In contrast to classification, where the output is a discrete category, regression models predict a numeric value based on the input data. Regression can be applied to a dataset as long as the dataset supports supervised learning and contains continuous numerical values. To train a regression model, we use a labeled dataset and then use the trained model to make predictions on new, unlabeled data. This allows us to predict the value of a continuous variable based on the patterns learned from the training data ~\cite{james-statistical-learning1}. 

The training data consists of input and class label pairs. The input can be a single value, such as numbers or strings, or vectors of values, such as a list of numbers or strings. The input can also be a matrix of values, such as a grayscale or color image. The class label is a discrete value, such as a number or a string. In this project, a class label is a number representing the digit in the image~\cite{james-statistical-learning1}.

The model is trained to minimize the error between the predicted and actual class labels. The error is calculated by comparing the predicted class label with the actual class label~\cite{james-statistical-learning1}.

The model is trained by finding the best parameters for the model, such as weights in a neural network or parameters in models, such as \gls{svm}~\cite{james-statistical-learning1}. These parameters help the model predict the category of new data. Hyperparameter optimization is finding the best parameters; this is discussed further in Section~\ref{sec:hyperparam}.

One must consider the data and the problem when deciding which model to use. The model must be able to handle the data and the problem. In this project, the \gls{ml} model is used for image classification, as the \gls{mnist} dataset is used. Several \gls{ml} models have been used for image classification with \gls{mnist} in general; the ones the group has looked into is~\cite{lecun-mnist-database,IBM-computer-vision,convolutional-neural-networks-convnets,multi-column-neural-network-ciregan}. These models can be used to classify images into one of ten categories, representing the digits 0-9, as this is the project's focus.

\subsubsection{Support Vector Machine}\label{subsubsec:support-vector-machine}
\gls{svm} is a supervised \gls{ml} model that can be used for classification or regression problems. It is a linear model for classification and regression problems. It is a binary classifier, meaning it can only classify two classes. It is a linear classifier, meaning it makes a decision based on a linear function of the input features~\cite{james-statistical-learning1}.

\gls{svm} classifies data by finding a mapping (hyperplane) that separates the classes in data~\cite{faster-svm}. \gls{svm} is also known as a large-margin classifier, which means that it relies on finding \textcquote{faster-svm}{a maximum-margin hyperplane to separate classes}. As the name implies, \gls{svm} uses support vectors, which are the 'vectors' closest to the function defining the mapping, and alterations on those data points will influence the hyperplane.
An exciting property of \gls{svm} is that it can have, among other parameters, a kernel function, which can be tuned whether the data is linearly separable or not~\cite{faster-svm}. The kernel function that \gls{svm} resembles the way \gls{kpca} works, as it also uses a kernel function.

For the project's purposes, \gls{svm} is a promising model since \textcquote{james-statistical-learning1}{SVMs have been shown to perform well in a variety of settings and are often considered one of the best "out of the box" classifiers.}. This is because \gls{svm} is a linear model, which means it is fast to train and predict. This is important for the project, as the model needs to classify the images quickly. The model also has a high accuracy, which is vital for the project, as the model needs to classify the images correctly. Though it is not as accurate as other models, such as \gls{nn}, it is still a good model for the project. On the downside of \gls{svm}, \cite{james-statistical-learning1} states \textcquote{james-statistical-learning1}{Though it is elegant and simple, we will see that this classifier, unfortunately, cannot be applied to most data sets, since it requires that the classes be separable by a linear boundary}. It means there can be some errors in the \gls{ml} model, as it is only sometimes possible to separate the classes in \gls{mnist} by a linear boundary; this is something the group has considered choosing the model.

This project is not mainly concerned with the choice of \gls{ml} model but rather with the choice of dimensionality reduction methods. Therefore, \gls{svm} is chosen as the \gls{ml} model as it has already been used with \gls{mnist} without dimensionality reduction~\cite{lecun-mnist-database}.



\subsection{Multi-class classification}\label{subsec:multi-class}
In classification, there are two types of classification, binary classification, and multi-class classification. Binary classification is the classification of two classes, while multi-class classification is the classification of more than two classes. The \gls{mnist} dataset used in this project presents a multi-class classification problem, as the images can represent any of the ten digits. The \gls{svm} model, however, is a binary classification model and thus has to be adapted to the multi-class classification problem. there will explained two approaches to this problem: \gls{ovo} and \gls{ova}~\cite{james-statistical-learning1}.

\subsubsection{One-vs-One}\label{subsubsec:one-vs-one}
\gls{ovo} is a method where the model trains on all possible combinations of two classes. For example, if there are five classes, the model is trained on ten different models, one for each combination of two classes; this makes it computationally expensive as it has to go through every combination. Then the model is evaluated against all other models, and the class with the highest score is picked as the predicted class~\cite{james-statistical-learning1}.

\subsubsection{One-vs-All}\label{subsubsec:one-vs-all}
However, \gls{ova} is faster than \gls{ovo}, as it only uses one class to distinguish if the data is similar. For example, if there are five classes, the model is trained on five variations of the model, one for each class. This makes \gls{ova} good to distinguish between the current class that is being modeled from the other classes. However, in OVA, it is harder to distinguish between the other classes that are not being trained. Then the model is then evaluated on all models, and the class with the highest score is chosen as the predicted class~\cite{james-statistical-learning1}.

\subsubsection{One-vs-One vs One-vs-All}\label{subsubsec:one-vs-one-vs-one-vs-all}
\gls{ovo} is chosen as the method for multi-class classification. This is due to that in scikit-learn library, \gls{ovo} is the default approach for multi-class classification using the \gls{svm} model. 