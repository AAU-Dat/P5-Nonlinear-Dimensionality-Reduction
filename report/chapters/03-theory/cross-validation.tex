\section{Cross-validation}\label{sec:cross-validation}
This section will describe cross-validation, why it is used in general and why it is used in this project.

A common problem in machine learning is that the test set is supposed to be used only in the final evaluation. This problem makes it hard to, for example, hyperparameter tune or chooses the correct model for the data. On the other hand, if the test data is used to tune or choose a model, this often leads to overfitting~\cite{james-statistical-learning}.

Overfitting can be described as when the model is very good at predicting the test data but will not do well with new data, not from the test set. This section will introduce one technique that will reduce the chances of overfitting the model: cross-validation.

Cross-validation is a technique that splits the training data into two sets: new training data set and a validation data set(comprising of training data). By partitioning the training data into two sets, the model can learn from the training data and evaluate on the validation data. The model can be tuned with the validation data and evaluated on the test data after the model is optimized. Such an approach is practical because the model gets evaluated based on data it has never seen, which shows how good the model is at predicting data it has not yet seen~\cite{scikit-learn-ml}. Cross-validation is particularly useful with hyperparameter tuning because it allows picking the correct parameters without using the test set. Cross-validation, more practically can be described as

\blockcquote{Refaeilzadeh2009}{The basic form of cross-validation is the basic form of k-fold cross-validation. (. . .) In k-fold cross-validation, the data is first partitioned into $k$ equally (or nearly equally) sized segments or folds. Subsequently, $k$ iterations of training and validation are performed such that within each iteration a different fold of the data is held-out for validation while the remaining $k-1$ folds are used for learning.}

There are also exists other forms of cross-validation, which are special cases of k-fold. As described before, shuffling the data might be necessary, and there is a version of k-fold called Stratified k-fold, where the data gets split into equal sets in regards to the total of each class for each round. Alternatively, instead of stratifying the data, one can use the leave-one-out cross-validation, where for each fold, all of the data except one sample is used for training~\cite{Refaeilzadeh2009}.

Stratifying or shuffling the data may be necessary so that each fold has a balanced distribution of classes. There is a risk that the folds contain imbalanced samples for each class, which may affect the overall model performance because the model is biased towards the majority class~\cite{Refaeilzadeh2009}.

In order to avoid overfitting, one can use k-fold cross-validation or derivatives of it, but k-fold implies that a specific number must be chosen. The choice of $k$ could be made through trial and error. However, the train and data samples need to be large enough to be statistically representative of the data set~\cite{Refaeilzadeh2009}.

\input{chapters/03-theory/decisions/choice-of-cross-validation.tex}