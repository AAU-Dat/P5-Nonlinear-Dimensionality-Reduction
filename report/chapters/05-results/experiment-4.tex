\section{Experiment 4}\label{sec:experiment-4}

In this experiment, a machine learning pipeline is used to explore how the performance of linear and nonlinear dimensionality reduction methods compare using different sizes of samples.

Overall, the results of this experiment provide insight into the factors that can influence the effectiveness of dimensionality reduction in machine learning and can inform the choice of dimensionality reduction method in real-world scenarios. Furthermore, the findings can contribute to the broader understanding of the role of sample size in choices of dimensionality reduction and its effect on machine learning models, which is relevant especially when working with cross validation.

\subsection{Rules and overview of the experiment}
This experiment uses the original hyperparameters. These can also be read in \autoref{subsubsec:the-hyperparameters}. The difference is that it will be used on datasamples from \gls{mnist} of the size 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000.


\subsection{Results}\label{subsec:experiment_4_results}
This section presents the results of the fourth experiment, which compares the performance of different dimensionality reduction methods on a classification task. The results are shown for different sample sizes using confusion matrices, tables of the classification reports, and scatter plots, which visualize the relationship between sample size, accuracy, and time taken.

The results are evaluated based on the rules of the experiment, focusing on accuracy, F1 score, and time taken. The specific accuracy values obtained from the experiment's CSV files are also discussed. The scatter plots provide a visual representation of the results and enable the comparison of when the different methods accuracy and time start to rise. In general, the results for each method and the base case will show the results for samples of sizes of 200, 1000 and 5000 in detail.

\subsubsection{Support vector machine model}\label{subsubsec:experiment_4_no_dimmensionality_reduction}


\input{figures/experiment-4/classification_report_baseline_svm_200.tex}
\input{figures/experiment-4/classification_report_baseline_svm_1000.tex}
\input{figures/experiment-4/classification_report_baseline_svm_5000.tex}

The results of this experiment are shown in \autoref{tab:classification-report-baseline_svm_200}. The table shows the precision, recall, and f1-score for each of the ten classes in the dataset and the overall accuracy of the model. The table also shows the macro average and weighted average scores.

Overall, the results show that the \gls{svm} model achieved an accuracy of approximately 67\%. This indicates that the model performed relatively well but still had some errors in its predictions. The precision and recall scores for each class varied, with some classes having higher scores than others. For example, the model had a precision of approximately 87\% for class 0 but only a precision of approximately 45\% for class 9.

These results provide a baseline for comparison with the results of the other parts of the experiment, in which different dimensionality reduction methods were applied. The results of this initial experiment will be used to evaluate the effectiveness of the different dimensionality reduction methods in improving the performance of the \gls{svm} model.

The second part of the base case involved using a sample size of 1000; it can be seen in \ref{tab:classification-report-baseline_svm_1000}. Compared to the results of the first part of the experiment, the accuracy of the \gls{svm} model improved significantly, achieving an accuracy of approximately 87\%. This indicates that using a larger sample size improved the performance of the model. The precision and recall scores for each class also improved, with most classes having higher scores than in the first part of the experiment.

The following sample of size 5000 can be seen in \autoref{tab:classification-report-baseline_svm_5000}. Comparing \autoref{tab:classification-report-baseline_svm_5000} and \autoref{tab:classification-report-baseline_svm_5000}, it is clear that the \gls{svm} model achieved higher accuracy and precision when trained on a larger sample size. For example, the accuracy of the model increased from 87\% for the 1000-sample dataset to 92\% for the 5000-sample dataset. Similarly, the precision of the model increased for most of the classes, with the most significant increase being seen for class 5, where the precision increased from 78\% to 90\%.

Additionally, the f1-score, which is a measure of the balance between precision and recall, also improved for most classes when using a larger sample size. This suggests that the model was able to make more accurate predictions and avoid false positives and false negatives more effectively when trained on a larger dataset.


\subsubsection{Principal component analysis}\label{subsubsec:experiment_4_pca}

The second part of the experiment involved running a machine learning pipeline applying \gls{pca} as a dimensionality reduction method. The sample sizes for this part of the experiment were 200, 1000, and 5000.

\input{figures/experiment-4/classification_report_pca_svm_200.tex}

This classification report shows the performance of a \gls{svm} model that has been trained using \gls{pca}. The result of the first sample of 200 can be seen in \ref{tab:classification-report-pca_svm_200}
For example, class 0 has a precision of 84\% and a recall of 88\%, while class 9 has a precision of 64\% and a recall of 72\%. This indicates that the model is more accurate at correctly identifying instances of class 0 than it is at correctly identifying instances of class 9. The f1-score for class 0 is 86\%, while the f1-score for class 9 is 68\%, further highlighting the difference in performance between the two classes. Overall, it appears that class 0 and class 9 have the largest differences in performance on this classification report.

\input{figures/experiment-4/classification_report_pca_svm_1000.tex}

In \ref{tab:classification-report-pca_svm_1000} it can see that overall, the second model (pca\_svm\_1000) performs better on the classification task than the first model (pca\_svm\_200), as shown by the higher accuracy, precision, recall, and f1-score values for most classes in the second report. For example, class 0 has a precision of 91\% and a recall of 96\% in the second report, compared to 83\% and 88\% in the first report. The f1-score for class 0 is also higher in the second report (94\%) than in the first report (85\%).

One interesting difference between the two reports is the performance on class 3. In the first report, class 3 has a recall of 83\%, with an f1-score of 78\%. In the second report, class 3 had a lower recall of 79\%, resulting in a f1-score of 83\%. This indicates that the second model (pca\_svm\_1000) is less accurate at correctly identifying instances of class 3 than the first model the f1 score is still higher due to the precision being suitably higher to compensate(pca\_svm\_200).

The last model is seen in \autoref{tab:classification-report-pca_svm_5000} and is overall, the pca\_svm\_5000 model appears to have better performance across most of the metrics, with higher values for precision, recall, and f1-score for most of the classes.

\input{figures/experiment-4/classification_report_pca_svm_5000.tex}


Overall, the results show that the \gls{svm} model using \gls{pca} achieved an accuracy of approximately 75\%, 77\%, and 84\% for the 200, 1000, and 5000 sample sizes, respectively. This indicates that the model performed relatively well, but still had some errors in its predictions. The precision and recall scores for each class varied, with some classes having higher scores than others. For example, the model had a precision of approximately 83\% for class 0 with a 200 sample size, but only a precision of approximately 64\% for class 9 with a 1000 sample size. Classes 0 and 9 showed the most significant differences in performance across the different sample sizes.

\subsubsection{Linear discriminant analysis}\label{subsubsec:experiment_4_lda}

\input{figures/experiment-4/classification_report_lda_svm_200.tex}


This classification report seen in \autoref{tab:classification-report-lda_svm_200} shows the performance of a \gls{svm} model that has been trained using \gls{lda} on classifying handwritten digits from the \gls{mnist} dataset. For example, class 1 has a precision of 60\% and a recall of 94\%, while class 5 has a precision of 48\% and a recall of 20\%. This indicates that the model is more accurate at correctly identifying instances of class 1 than it is at correctly identifying instances of class 5. The f1-score for class 1 is 73\%, while the f1-score for class 5 is 28\%, further highlighting the difference in performance between the two classes. Overall, it appears that class 1 and class 5 have the largest differences in performance on this classification report.

\input{figures/experiment-4/classification_report_lda_svm_1000.tex}

The classification report for lda\_svm\_1000 seen in \autoref{tab:classification-report-lda_svm_1000} shows generally better performance than the classification report for lda\_svm\_200. For example, class 1 has a precision of 74\% and a recall of 94\% in the lda\_svm\_1000 report, compared to a precision of 60\% and a recall of 94\% in the lda\_svm\_200 report. Additionally, the f1-score for class 1 is 83\% in the lda\_svm\_1000 report, compared to 73\% in the lda\_svm\_200 report. This indicates that the model trained on a larger sample size of 1000 has improved performance in correctly identifying instances of class 1. Overall, it appears that several classes, including 1, 3, 5, and 9, have seen improvements in precision, recall, and f1-score when trained on a larger sample size.


\input{figures/experiment-4/classification_report_lda_svm_5000.tex}

The classification report for lda\_svm\_5000 has higher precision, recall, and f1-score values for each class compared to the lda\_svm\_1000 report. For example, the precision for class 0 is 92\% in the lda\_svm\_5000 report, while it is 69\% in the lda\_svm\_1000 report. Similarly, the recall for class 0 is 95\% in the lda\_svm\_5000 report, while it is 81\% in the lda\_svm\_1000 report. The f1-score for class 0 is also higher in the lda\_svm\_5000 report (93\%) compared to the lda\_svm\_1000 report (75\%). Overall, it appears that the model trained on a larger sample size is more effective at correctly classifying instances in the \gls{mnist} dataset.

Based on these classification reports, the model with \gls{lda} is best at recognizing instances of class 1. This is because it has the highest precision and recall among all classes, as well as the highest f1-score. This indicates that the model is able to accurately identify instances of class 1 with a high degree of precision and recall. Furthermore, the difference in performance between class 1 and other classes is the largest, further highlighting the model's superior performance on this class. It also appears that the model is worst at recognizing classes 8, 9, 2 and 5.

\subsubsection{Kernel PCA}\label{subsubsec:experiment_4_kernel_pca}
In this part of the experiment it is expected that The performance of a \gls{svm} model using  \gls{kpca} for dimensionality reduction is likely to differ from that of an \gls{svm} model without \gls{kpca}. \gls{kpca} can reduce the dimensionality of a dataset by projecting it onto a lower-dimensional space, which can improve the \gls{svm} model's decision boundary and performance. In contrast, an \gls{svm} model without \gls{kpca} may be more sensitive to the curse of dimensionality and overfitting, especially on high-dimensional datasets.

\input{figures/experiment-4/classification_report_kernel_pca_svm_200.tex}


The values in \autoref{tab:classification-report-kernel_pca_svm_200} indicate that the model has relatively high precision and recall for most classes, with a few exceptions. Overall, the model has an accuracy of approximately 74\%.

\input{figures/experiment-4/classification_report_kernel_pca_svm_1000.tex}

The classification report for kernel\_pca\_svm\_200 and kernel\_pca\_svm\_1000 show differences in the performance of the model on the two datasets. In general, the model trained on the larger dataset (kernel\_pca\_svm\_1000) which can be seen in \autoref{tab:classification-report-kernel_pca_svm_1000} appears to have higher precision, recall, and f1-scores for most classes.

\input{figures/experiment-4/classification_report_kernel_pca_svm_5000.tex}

In general, the model trained on the larger dataset (kernel\_pca\_svm\_5000) which can be seen in \autoref{tab:classification-report-kernel_pca_svm_5000} appears to have higher precision, recall, and f1-scores for most classes. This suggests that, in this case, increasing the size of the dataset has led to a more accurate model.

Looking at the individual classes, some of the largest differences in performance can be seen in classes 1, 4, and 6. For class 1, the model trained on kernel\_pca\_svm\_5000 has a precision of 95\%, a recall of 97\%, and an f1-score of 96\%, while the model trained on kernel\_pca\_svm\_1000 has a precision of 92\%, a recall of 97\%, and an f1-score of 94\%. This indicates that the larger model is more effective at correctly identifying and classifying examples from class 1.

For class 4, the model trained on kernel\_pca\_svm\_5000 has a precision of 90\%, a recall of 93\%, and an f1-score of 91\%, while the model trained on kernel\_pca\_svm\_1000 has a precision of 84\%, a recall of 88\%, and an f1-score of 86\%. This indicates that the larger model is more effective at correctly identifying and classifying examples from class 4.

For class 6, the model trained on kernel\_pca\_svm\_5000 has a precision of 92\%, a recall of 93\%, and an f1-score of 93\%, while the model trained on kernel\_pca\_svm\_1000 has a precision of 90\%, a recall of 90\%, and an f1-score of 90\%. This indicates that the larger model is more effective at correctly identifying and classifying examples from class 6.

\subsubsection{ISOMAP embedding}\label{subsubsec:experiment_4_isomap}
This section presents the results of an experiment that was conducted to investigate the effects of sample size on the performance of \gls{isomap} and \gls{svm}. In this experiment, a set of data points representing a particular problem or phenomenon was divided into multiple groups, each containing a different number of samples.

\input{figures/experiment-4/classification_report_isomap_svm_200.tex}

\autoref{tab:classification-report-isomap_svm_200} shows the evaluation metrics for a classifier trained on 500 instances of the MNIST dataset. The classifier has an overall accuracy of 69\% and an f1-score ranging from 48\% to 83\%. The classifier performs relatively poorly across all classes, with f1-scores below 80\% for all classes except 1 and 7.

\input{figures/experiment-4/classification_report_isomap_svm_1000.tex}

\autoref{tab:classification-report-isomap_svm_1000} shows the evaluation metrics for a classifier trained on 1000 instances of the MNIST dataset. The classifier has an overall accuracy of 77\% and an f1-score ranging from 63\% to 89\%. The classifier performs well on classes 1, 6, and 7, but relatively poorly on class 5, with an f1-score of 64\%.


\input{figures/experiment-4/classification_report_isomap_svm_5000.tex}


\autoref{tab:classification-report-isomap_svm_5000} shows the evaluation metrics for a classifier trained on 5000 instances of the MNIST dataset. The classifier has an overall accuracy of 88\% and an f1-score ranging from 79\% to 93\%. The classifier performs particularly well on classes 1, 6, and 7, with f1-scores above 90\%.


The figures above show that in general, it appears that increasing the number of components in the Isomap technique leads to an improvement in the model's performance. In the classification report for isomap\_svm\_5000, the model has the highest average f1-score of 87\%, compared to 77\% in isomap\_svm\_1000 and 72\% in isomap\_svm\_200. This trend is also seen in other evaluation metrics, such as precision and recall.

Additionally, it can be  seen that the model's performance varies across the different classes in the dataset. For example, in isomap\_svm\_5000, the model has a high f1-score for classes 1, 6, and 7, but a relatively low f1-score for class 2.


\subsection{Discussion of experiment 4}
In terms of performance, \autoref{fig:experiment_4_performance_size} shows that both baseline \gls{pca} and \gls{kpca} perform similarly well across all sample sizes, with consistently high F1 scores. In contrast, \gls{lda} performs worse overall, while \gls{isomap} has slightly lower F1 scores in smaller sample sizes but performs better in larger samples.


\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/test_score_based_on_size.png}
    \caption{}
    \label{fig:experiment_4_performance_size}
\end{figure}


\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/time_based_on_size.png}
    \caption{}
    \label{fig:experiment_4_speed_size}
\end{figure}


In terms of performance, \autoref{fig:experiment_4_performance_size} shows that both baseline \gls{pca} and \gls{kpca} perform similarly well across all sample sizes, with consistently high F1 scores. In contrast, \gls{lda} performs worse overall, while \gls{isomap} has slightly lower F1 scores in smaller sample sizes but performs better in larger samples.

In terms of speed, \autoref{fig:experiment_4_speed_size} shows that both \gls{pca} and \gls{lda} are the fastest when the sample size is larger than 2000, although they are slightly slower in smaller samples. Baseline \gls{pca} is the third slowest, while \gls{kpca} is the second slowest at a sample size of 5000. \gls{isomap} is the slowest overall, with longer runtimes for sample sizes of 2000 and above.

Overall, it appears that both \gls{pca} and \gls{kpca} are good choices for improving the performance of a \gls{svm} model on the \gls{mnist} dataset, as they offer both high performance and fast runtime. \gls{lda} may be a less optimal choice due to its lower performance, while \gls{isomap} may be a bad option for larger sample sizes but may be suitable for smaller samples due to its slower runtime.
