
\section{Non-linear methods}\label{sec:non-linear-methods}
Sometimes the high-dimensional data may contain nonlinear relationships. Some linear methods may fail to find features in data that are nonlinear. In \cite{Tennenbaum} it is shown that \gls{pca} "sees" only the Euclidean structure of a manifold (Swiss roll data set), and thus fails to see the geodesic distances -- the distance between two points on the manifold -- that actually represent the structure of the manifold. A manifold can be thought of as being a space that locally resembles Euclidean space.

On the other hand, one can handle such data with kernel methods instead of considering the data a manifold. A kernel method can be used to project the data onto a higher dimension that could resemble a linear plane, or Euclidean space, which would make it easier to use a linear method \cite{LVD}.

This section will provide some nonlinear methods that are able to reduce the dimensionality of nonlinear data.


\subsection{Kernel PCA}\label{subsec:kernel-pca}
\gls{kpca} is an extension of \gls{pca}, and it projects the data onto a higer dimensional plane, where \gls{pca} is performed. \gls{kpca} uses a kernel function $\phi (x)$ which is used to obtain to what would correspond as the covariance matrix in \gls{pca}in the kernel space $K$, from which eigenvalue decomposition can be computed, without computing the actual kernel function $\phi (x)$, which otherwise would be computationally expensive. Thus, \gls{kpca} applies a kernel function (some examples being Guassian or polynomial kernels) to the data set $ \{x_{i} \}$, constructs a kernel matrix. From the kernel matrix one can compute the Gram matrix, from which, similar to \gls{pca}, one can make use of eigen-decomposition to get the kernel $k$ components \cite{kernel-pca}.


\subsection{Isometric feature mapping}\label{subsec:isometric-feature-mapping}
\gls{isomap} is another non-linear method that is a special case of \gls{cmds}. The former method preserves the geodesic distance, while the latter performs dimensionality reduction, which is why \gls{isomap} can be considered an extension of \gls{mds}. The geodesic distance is calculated by first constructing a neighborhood graph for every data point connected to its $k$ point, which is done with K-nearest neighbors algorithm. After constructing the neighborhood graph, the distance then can be calculated with a shortest path algorithm to determine the distances on the graph which correspond to the distances on the manifold. This results in a distance matrix, which is used as the input for \gls{cmds} multi-dimensional-scaling-and-isomap.




\gls{cmds} tries to preserve the similarity between the data points from the higher dimensional space that are to be embedded onto the lower dimensional space. This is achieved by performing eigen-decomposition on the matrix input. \gls{cmds} resembles \gls{pca} -- one can think of \gls{mds} as being a family of methods while \gls{pca} is just a method \cite{difference-between-pca-and-mds} multi-dimensional-scaling-and-isomap.



It is assumed that \gls{isomap} is suited to discover manifolds of arbirary dimensionality, and that it guarantees an approximation of the true structure of the manifold \cite{Tennenbaum}.